{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efa84833",
   "metadata": {},
   "source": [
    "# Model Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfe48aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PACKAGES IMPORTS ###\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tcn import TCN\n",
    "import optuna\n",
    "from optuna import Trial\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "\n",
    "### INCLUDE GRAPHS IN NOTEBOOK AUTOMATICALLY ###\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2167dfbc",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1b3ae3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe:  (6832, 22)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>flight</th>\n",
       "      <th>drone_type</th>\n",
       "      <th>v_N</th>\n",
       "      <th>v_E</th>\n",
       "      <th>v_D</th>\n",
       "      <th>a_N</th>\n",
       "      <th>a_E</th>\n",
       "      <th>a_D</th>\n",
       "      <th>wind_N</th>\n",
       "      <th>...</th>\n",
       "      <th>battery_power</th>\n",
       "      <th>alt</th>\n",
       "      <th>long</th>\n",
       "      <th>lat</th>\n",
       "      <th>phi</th>\n",
       "      <th>theta</th>\n",
       "      <th>psi</th>\n",
       "      <th>angular_x</th>\n",
       "      <th>angular_y</th>\n",
       "      <th>angular_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>matrice_100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>269.332402</td>\n",
       "      <td>-79.782396</td>\n",
       "      <td>40.458047</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.009941</td>\n",
       "      <td>-0.516789</td>\n",
       "      <td>-0.047280</td>\n",
       "      <td>0.080640</td>\n",
       "      <td>0.390483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.810</td>\n",
       "      <td>1</td>\n",
       "      <td>matrice_100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>269.335937</td>\n",
       "      <td>-79.782396</td>\n",
       "      <td>40.458047</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.009950</td>\n",
       "      <td>-0.516782</td>\n",
       "      <td>-0.232598</td>\n",
       "      <td>0.058941</td>\n",
       "      <td>-0.145898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.500</td>\n",
       "      <td>1</td>\n",
       "      <td>matrice_100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>269.332274</td>\n",
       "      <td>-79.782397</td>\n",
       "      <td>40.458047</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>0.009934</td>\n",
       "      <td>-0.516757</td>\n",
       "      <td>0.030549</td>\n",
       "      <td>0.091090</td>\n",
       "      <td>-0.358082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.300</td>\n",
       "      <td>1</td>\n",
       "      <td>matrice_100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>269.331324</td>\n",
       "      <td>-79.782397</td>\n",
       "      <td>40.458047</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.009938</td>\n",
       "      <td>-0.516745</td>\n",
       "      <td>-0.038736</td>\n",
       "      <td>0.151272</td>\n",
       "      <td>0.349120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.000</td>\n",
       "      <td>1</td>\n",
       "      <td>matrice_100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>269.330575</td>\n",
       "      <td>-79.782397</td>\n",
       "      <td>40.458048</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.009939</td>\n",
       "      <td>-0.516732</td>\n",
       "      <td>0.081788</td>\n",
       "      <td>0.049108</td>\n",
       "      <td>0.186339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6827</th>\n",
       "      <td>445.858</td>\n",
       "      <td>501</td>\n",
       "      <td>inspire</td>\n",
       "      <td>-0.016295</td>\n",
       "      <td>-0.047314</td>\n",
       "      <td>0.923826</td>\n",
       "      <td>0.005702</td>\n",
       "      <td>-0.013041</td>\n",
       "      <td>-1.065106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>342.151992</td>\n",
       "      <td>92.801610</td>\n",
       "      <td>103.683285</td>\n",
       "      <td>1.344939</td>\n",
       "      <td>1.523057</td>\n",
       "      <td>-0.082441</td>\n",
       "      <td>-151.844488</td>\n",
       "      <td>8.974604</td>\n",
       "      <td>-5.662300</td>\n",
       "      <td>-3.522902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6828</th>\n",
       "      <td>446.858</td>\n",
       "      <td>501</td>\n",
       "      <td>inspire</td>\n",
       "      <td>0.025509</td>\n",
       "      <td>-0.027040</td>\n",
       "      <td>0.356245</td>\n",
       "      <td>-0.002888</td>\n",
       "      <td>-0.006615</td>\n",
       "      <td>-1.101281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>340.677040</td>\n",
       "      <td>92.021736</td>\n",
       "      <td>103.683285</td>\n",
       "      <td>1.344939</td>\n",
       "      <td>0.956871</td>\n",
       "      <td>-0.789295</td>\n",
       "      <td>-155.280442</td>\n",
       "      <td>-1.846924</td>\n",
       "      <td>0.216854</td>\n",
       "      <td>-3.134087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6829</th>\n",
       "      <td>447.858</td>\n",
       "      <td>501</td>\n",
       "      <td>inspire</td>\n",
       "      <td>0.039037</td>\n",
       "      <td>-0.101185</td>\n",
       "      <td>0.012075</td>\n",
       "      <td>0.004992</td>\n",
       "      <td>-0.002673</td>\n",
       "      <td>-1.033789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>340.677040</td>\n",
       "      <td>91.849470</td>\n",
       "      <td>103.683284</td>\n",
       "      <td>1.344938</td>\n",
       "      <td>0.323613</td>\n",
       "      <td>1.214056</td>\n",
       "      <td>-151.855352</td>\n",
       "      <td>-0.667574</td>\n",
       "      <td>-1.356790</td>\n",
       "      <td>10.510001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6830</th>\n",
       "      <td>448.858</td>\n",
       "      <td>501</td>\n",
       "      <td>inspire</td>\n",
       "      <td>0.003986</td>\n",
       "      <td>-0.048694</td>\n",
       "      <td>0.073468</td>\n",
       "      <td>0.005926</td>\n",
       "      <td>-0.002943</td>\n",
       "      <td>-1.054360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>387.050224</td>\n",
       "      <td>91.655090</td>\n",
       "      <td>103.683283</td>\n",
       "      <td>1.344938</td>\n",
       "      <td>-0.770386</td>\n",
       "      <td>0.245185</td>\n",
       "      <td>-149.771986</td>\n",
       "      <td>20.306923</td>\n",
       "      <td>4.552239</td>\n",
       "      <td>-0.612599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6831</th>\n",
       "      <td>449.858</td>\n",
       "      <td>501</td>\n",
       "      <td>inspire</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>-0.105908</td>\n",
       "      <td>0.055013</td>\n",
       "      <td>-0.014138</td>\n",
       "      <td>-1.033002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>355.992818</td>\n",
       "      <td>91.555640</td>\n",
       "      <td>103.683282</td>\n",
       "      <td>1.344938</td>\n",
       "      <td>0.600086</td>\n",
       "      <td>3.272731</td>\n",
       "      <td>-150.170388</td>\n",
       "      <td>-0.392680</td>\n",
       "      <td>0.770688</td>\n",
       "      <td>0.030295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6832 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         time  flight   drone_type       v_N       v_E       v_D       a_N  \\\n",
       "0       0.000       1  matrice_100  0.000000  0.000000  0.000000  0.000000   \n",
       "1       0.810       1  matrice_100  0.000000  0.000000  0.000000  0.000000   \n",
       "2       1.500       1  matrice_100  0.000000  0.000000  0.000000  0.000000   \n",
       "3       2.300       1  matrice_100  0.000000  0.000000  0.000000  0.000000   \n",
       "4       3.000       1  matrice_100  0.000000  0.000000  0.000000  0.000000   \n",
       "...       ...     ...          ...       ...       ...       ...       ...   \n",
       "6827  445.858     501      inspire -0.016295 -0.047314  0.923826  0.005702   \n",
       "6828  446.858     501      inspire  0.025509 -0.027040  0.356245 -0.002888   \n",
       "6829  447.858     501      inspire  0.039037 -0.101185  0.012075  0.004992   \n",
       "6830  448.858     501      inspire  0.003986 -0.048694  0.073468  0.005926   \n",
       "6831  449.858     501      inspire  0.000997  0.018200 -0.105908  0.055013   \n",
       "\n",
       "           a_E       a_D  wind_N  ...  battery_power         alt        long  \\\n",
       "0     0.000000 -1.000000     0.0  ...       0.000000  269.332402  -79.782396   \n",
       "1     0.000000 -1.000000     0.0  ...       0.000000  269.335937  -79.782396   \n",
       "2     0.000000 -1.000000     0.0  ...       0.000000  269.332274  -79.782397   \n",
       "3     0.000000 -1.000000     0.0  ...       0.000000  269.331324  -79.782397   \n",
       "4     0.000000 -1.000000     0.0  ...       0.000000  269.330575  -79.782397   \n",
       "...        ...       ...     ...  ...            ...         ...         ...   \n",
       "6827 -0.013041 -1.065106     0.0  ...     342.151992   92.801610  103.683285   \n",
       "6828 -0.006615 -1.101281     0.0  ...     340.677040   92.021736  103.683285   \n",
       "6829 -0.002673 -1.033789     0.0  ...     340.677040   91.849470  103.683284   \n",
       "6830 -0.002943 -1.054360     0.0  ...     387.050224   91.655090  103.683283   \n",
       "6831 -0.014138 -1.033002     0.0  ...     355.992818   91.555640  103.683282   \n",
       "\n",
       "            lat       phi     theta         psi  angular_x  angular_y  \\\n",
       "0     40.458047  0.000560  0.009941   -0.516789  -0.047280   0.080640   \n",
       "1     40.458047  0.000561  0.009950   -0.516782  -0.232598   0.058941   \n",
       "2     40.458047  0.000571  0.009934   -0.516757   0.030549   0.091090   \n",
       "3     40.458047  0.000576  0.009938   -0.516745  -0.038736   0.151272   \n",
       "4     40.458048  0.000587  0.009939   -0.516732   0.081788   0.049108   \n",
       "...         ...       ...       ...         ...        ...        ...   \n",
       "6827   1.344939  1.523057 -0.082441 -151.844488   8.974604  -5.662300   \n",
       "6828   1.344939  0.956871 -0.789295 -155.280442  -1.846924   0.216854   \n",
       "6829   1.344938  0.323613  1.214056 -151.855352  -0.667574  -1.356790   \n",
       "6830   1.344938 -0.770386  0.245185 -149.771986  20.306923   4.552239   \n",
       "6831   1.344938  0.600086  3.272731 -150.170388  -0.392680   0.770688   \n",
       "\n",
       "      angular_z  \n",
       "0      0.390483  \n",
       "1     -0.145898  \n",
       "2     -0.358082  \n",
       "3      0.349120  \n",
       "4      0.186339  \n",
       "...         ...  \n",
       "6827  -3.522902  \n",
       "6828  -3.134087  \n",
       "6829  10.510001  \n",
       "6830  -0.612599  \n",
       "6831   0.030295  \n",
       "\n",
       "[6832 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load csv file\n",
    "raw_data = pd.read_csv('C:/Users/nghj7/Desktop/Flight data/filtered_data.csv');\n",
    "#create dataframe\n",
    "df = pd.DataFrame(raw_data);\n",
    "print('Shape of dataframe: ', df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fef6848",
   "metadata": {},
   "source": [
    "## Model Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d37c053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features =  ['v_N','v_E','v_D',\n",
    "             'a_N','a_E','a_D',\n",
    "             'wind_N', 'wind_E',\n",
    "             'total_mass']\n",
    "labels = ['battery_power']\n",
    "test_seed = 1111\n",
    "valid_seed = 2222"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f7bd66",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0e8f667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (4372, 60, 9)\n",
      "Y_train shape: (4372, 1)\n",
      "X_valid shape: (1093, 60, 9)\n",
      "Y_valid shape: (1093, 1)\n",
      "X_test shape: (1367, 60, 9)\n",
      "Y_test shape: (1367, 1)\n"
     ]
    }
   ],
   "source": [
    "def custom_data_split(dataframe, time_step):\n",
    "    \"\"\"\n",
    "    This function takes the filtered dataframe, time step n as argument.\n",
    "    Returns the train, test, validation and drone list. \n",
    "    \n",
    "    Output dimension will be (BATCH, TIMESTEP, FEATURES)\n",
    "    Pre-padding for all earlier time steps: [-1e9, -1e9, 0, 1, 2, 3..]\n",
    "    Post-padding to make sure BATCH is divisible by batch size\n",
    "    \"\"\"    \n",
    "    flight_list = np.unique(dataframe['flight'])\n",
    "    x_data = []\n",
    "    for flno in flight_list:\n",
    "        #filter flight no\n",
    "        temp = dataframe[dataframe['flight'] == flno].copy()\n",
    "        x_temp = []\n",
    "        #apply pre-padding to TIME dimension\n",
    "        for i in range(1,time_step+1):\n",
    "            sliced = temp.iloc[0:i].copy()\n",
    "            x_slice = sliced[features].values.tolist()\n",
    "            x_padding = [-1e9] * len(features)\n",
    "            x_slice = [x_padding]*(time_step-i) + x_slice\n",
    "            x_temp.append(x_slice)\n",
    "        #remaining time step\n",
    "        remaining = [temp[i-time_step:i] for i in range(time_step, temp.shape[0])]\n",
    "        x_temp += [i[features].values.tolist() for i in remaining if i.shape[0] == time_step]\n",
    "        x_data.append(x_temp)\n",
    "    #reshape\n",
    "    x_data, y_data, all_drones = np.vstack(x_data), dataframe[labels], dataframe['drone_type']\n",
    "    #train-test split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2, stratify = all_drones, random_state = test_seed)\n",
    "    remaining_drones = all_drones.iloc[y_train.index]\n",
    "    #train-validation split\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size = 0.2, stratify = remaining_drones, random_state = valid_seed) \n",
    "    #reshape\n",
    "    y_train, y_valid, y_test = y_train.to_numpy(), y_valid.to_numpy(), y_test.to_numpy()\n",
    "    \n",
    "    return x_train, x_valid, x_test, y_train, y_valid, y_test\n",
    "\n",
    "#test function\n",
    "x_train, x_valid, x_test, y_train, y_valid, y_test = custom_data_split(df, 60)\n",
    "print('X_train shape:', x_train.shape)\n",
    "print('Y_train shape:', y_train.shape)\n",
    "print('X_valid shape:', x_valid.shape)\n",
    "print('Y_valid shape:', y_valid.shape)\n",
    "print('X_test shape:', x_test.shape)\n",
    "print('Y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6d3b79",
   "metadata": {},
   "source": [
    "## Functions to Create and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2292481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_name, params):\n",
    "    \"\"\"\n",
    "    Create the model based on params as a dictionary.\n",
    "    Returns the instantiated model\n",
    "    \"\"\"\n",
    "    #MULTILAYER PERCEPTRON\n",
    "    if model_name == 'mlp':\n",
    "        #create input layer\n",
    "        input_shape = (len(features),)\n",
    "        model =  Sequential([tf.keras.Input(shape = input_shape), \n",
    "                             BatchNormalization()])\n",
    "        #define kernel regularizer      \n",
    "        reg = tf.keras.regularizers.L1L2(l1 = params['reg_alpha'], l2 = params['reg_lambda'])\n",
    "        #add hidden layers\n",
    "        dropped = False\n",
    "        for i in range(params['n_layers']):\n",
    "            model.add(Dense(units = params['n_units_' + str(i)], activation = 'relu', kernel_regularizer = reg))\n",
    "            #add dropout layer\n",
    "            if not dropped and params['n_layers'] != 1: \n",
    "                model.add(Dropout(params['dropout_rate']))\n",
    "                dropped = True        \n",
    "        #output layer  \n",
    "        model.add(Dense(units=1, activation = 'linear'))\n",
    "        #compile model\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate = params['learning_rate'])\n",
    "        loss = params['loss']\n",
    "        metric = params['metric']\n",
    "        model.compile(optimizer = opt, loss = loss, metrics = [metric])\n",
    "        \n",
    "    #RECURRENT NETWORK\n",
    "    elif model_name in ['lstm', 'tcn']:\n",
    "        #define batch size, time step and num layers\n",
    "        batch_size = 128\n",
    "        time_step = params['time_step']\n",
    "        n_layers = params['n_layers']\n",
    "        #create input layer\n",
    "        batch_input_shape = (batch_size, time_step, len(features)-1)\n",
    "        time_input = tf.keras.Input(batch_shape = batch_input_shape, name ='time_variant')\n",
    "        fixed_input = tf.keras.Input(batch_shape = (batch_size,1), name = 'time_invariant')\n",
    "    \n",
    "        #masking layer\n",
    "        x = tf.keras.layers.Masking(mask_value=-1e9)(time_input)\n",
    "        x1 = tf.keras.layers.Masking(mask_value=-1e9)(fixed_input)\n",
    "        #create TCN\n",
    "        if model_name == 'tcn': \n",
    "            #define exponential dilation, kernel and filter size\n",
    "            dilations = [2**i for i in range(n_layers)]\n",
    "            kernel_size = params['kernel_size']\n",
    "            filter_size =  params['filter_size']\n",
    "            for i in range(n_layers):\n",
    "                if i != n_layers-1:\n",
    "                    #hidden layers\n",
    "                    x = TCN(nb_filters = filter_size, \n",
    "                            dilations = dilations, \n",
    "                            use_layer_norm = True, \n",
    "                            kernel_size = kernel_size,\n",
    "                            return_sequences = True)(x)\n",
    "                else:\n",
    "                    #last layer\n",
    "                    x = TCN(nb_filters = filter_size,\n",
    "                            dilations = dilations,\n",
    "                            use_layer_norm = True,\n",
    "                            kernel_size = kernel_size, \n",
    "                            return_sequences = False)(x)\n",
    "        #create LSTM\n",
    "        elif model_name == 'lstm':\n",
    "            #define kernel regularizer\n",
    "            reg = tf.keras.regularizers.L1L2(l1 = params['reg_alpha'], l2 = params['reg_lambda'])\n",
    "            for i in range(n_layers):\n",
    "                if i != n_layers-1:\n",
    "                    #hidden layers\n",
    "                    x = tf.keras.layers.LSTM(units = params['n_units_'+str(i)], return_sequences = True, kernel_regularizer= reg)(x)\n",
    "                else:\n",
    "                    #last layer\n",
    "                    x = tf.keras.layers.LSTM(units = params['n_units_'+str(i)], kernel_regularizer = reg)(x)\n",
    "        #concatenate time and non-time input\n",
    "        x_concat = tf.keras.layers.concatenate([x, x1])\n",
    "        #final dense layers\n",
    "        y = tf.keras.layers.Dense(10, activation='relu')(x_concat)\n",
    "        y = tf.keras.layers.Dense(1, activation= None)(y)\n",
    "        #create model\n",
    "        model = Model(inputs = [time_input, fixed_input], outputs = [y])\n",
    "        #define optimizer, loss function and metric\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate = params['learning_rate'])\n",
    "        loss = params['loss']\n",
    "        metric = params['metric']\n",
    "        #compile model\n",
    "        model.compile(optimizer = opt, loss = loss, metrics = [metric])\n",
    "        \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e62bfc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, model, params, x_train, x_valid, y_train, y_valid, trial):\n",
    "    \"\"\"\n",
    "    Trains the model using train and validation set.\n",
    "    Returns the trained model and loss history    \n",
    "    \"\"\"\n",
    "    #MULTILAYER PERCEPTRON\n",
    "    if model_name == 'mlp':\n",
    "        #batch calculation\n",
    "        batch_size = 128\n",
    "        #training calclation\n",
    "        n_train_examples = x_train.shape[0]\n",
    "        epoch = 800\n",
    "        steps_per_epoch = int(n_train_examples / batch_size) \n",
    "        #create train and validation set\n",
    "        dtrain = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size, drop_remainder = True)\n",
    "        dvalid = tf.data.Dataset.from_tensors((x_valid, y_valid))\n",
    "        #create callbacks to fit method\n",
    "        monitor = params['monitor']\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(mode='auto', patience = 20, monitor = monitor)\n",
    "        learning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor = monitor, factor=params['eta_decay'], \n",
    "                                                                       patience = 5, min_lr = 0.1)\n",
    "        #######\n",
    "        pruner =  optuna.integration.TFKerasPruningCallback(trial, monitor)\n",
    "        #######\n",
    "        callbacks = [learning_rate_reduction, early_stopping, pruner]\n",
    "        #fit model\n",
    "        history = model.fit(dtrain, epochs = epoch, steps_per_epoch = steps_per_epoch, \n",
    "                            callbacks = callbacks, validation_data = (x_valid, y_valid), verbose = 0)\n",
    "    \n",
    "    #RECURRENT NETWORKS\n",
    "    elif model_name in ['lstm', 'tcn']:\n",
    "        #batch calculation\n",
    "        batch_size = 128 \n",
    "        #training calculation\n",
    "        n_train_examples = x_train.shape[0]\n",
    "        epoch = 150\n",
    "        steps_per_epoch = int(n_train_examples / batch_size)\n",
    "        #create train set\n",
    "        train_time_input = x_train[:, :, :-1]\n",
    "        train_fixed_input = x_train[:, -1, -1].reshape(-1, 1)\n",
    "        x_train = {'time_variant':train_time_input, 'time_invariant':train_fixed_input}\n",
    "        dtrain = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size, drop_remainder = True)\n",
    "        #create validation set\n",
    "        valid_time_input = x_valid[:, :, :-1]\n",
    "        valid_fixed_input = x_valid[:, -1, -1].reshape(-1, 1)\n",
    "        x_valid = {'time_variant':valid_time_input, 'time_invariant':valid_fixed_input}\n",
    "        dvalid = tf.data.Dataset.from_tensor_slices((x_valid, y_valid)).batch(batch_size, drop_remainder = True)   \n",
    "        #create callbacks to fit method\n",
    "        monitor = params['monitor']\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(mode='auto', patience = 20, monitor = monitor)\n",
    "        learning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor = monitor, factor=params['eta_decay'], \n",
    "                                                                       patience =5, min_lr = 0.1)\n",
    "        #######\n",
    "        pruner =  optuna.integration.TFKerasPruningCallback(trial, monitor)\n",
    "        #######\n",
    "        callbacks = [learning_rate_reduction, early_stopping, pruner]    \n",
    "        #fit model\n",
    "        history = model.fit(dtrain, epochs = epoch, steps_per_epoch = steps_per_epoch, \n",
    "                            callbacks = callbacks, validation_data = dvalid, verbose = 0)\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69371fc",
   "metadata": {},
   "source": [
    "## Model Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b43cae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_objective(trial: Trial):\n",
    "    \"\"\"\n",
    "    This function takes the trial object from Optuna and uses the TPE algorithm\n",
    "    to sample the hyperparameters. Returns the history of validation score to \n",
    "    Optuna for pruning.\n",
    "    \"\"\"\n",
    "    params = {}\n",
    "    #PARAMS\n",
    "    if current_model == 'mlp':\n",
    "        params['n_layers'] = trial.suggest_int('n_layers', 1, 3)\n",
    "        params['reg_alpha'] = trial.suggest_float('reg_alpha', 1e-6, 1e-2, log=True)\n",
    "        params['reg_lambda'] = trial.suggest_float('reg_lambda', 1e-6, 1e-2, log=True)\n",
    "        params['dropout_rate'] = trial.suggest_float('dropout_rate', 0.1, 0.4, log=True)\n",
    "        for i in range(params['n_layers']):\n",
    "            params['n_units_'+str(i)] = trial.suggest_categorical(\"n_units_{}\".format(i), [32, 64, 128, 256])\n",
    "        params['learning_rate'] = trial.suggest_float('learning_rate', 1e-6, 1e-1, log = True)\n",
    "        params['eta_decay'] = trial.suggest_float('eta_decay', 0.5, 0.9)\n",
    "        #split train-test set\n",
    "        all_drones = df['drone_type']\n",
    "        x_train, x_test, y_train, y_test = train_test_split(df[features], df[labels], test_size = 0.2, stratify = all_drones, shuffle = True, random_state = test_seed)\n",
    "        remaining_drones = all_drones.loc[x_train.index]\n",
    "        #split validation set\n",
    "        x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size = 0.2, stratify = remaining_drones, shuffle = True, random_state = valid_seed)\n",
    "        \n",
    "    elif current_model == 'tcn':\n",
    "        params['time_step'] = trial.suggest_int('time_step', 1, 40)\n",
    "        params['n_layers'] = trial.suggest_int('n_layers', 2, 5)\n",
    "        params['kernel_size'] = trial.suggest_int('kernel_size', 2, 4)\n",
    "        params['filter_size'] = trial.suggest_categorical('filter_size', [16,32,64,128])\n",
    "        params['learning_rate'] = trial.suggest_float('learning_rate', 1e-6, 1e-1, log = True)\n",
    "        params['eta_decay'] = trial.suggest_float('eta_decay', 0.5, 0.9)\n",
    "        #split train-test set\n",
    "        x_train, x_valid, x_test, y_train, y_valid, y_test = custom_data_split(df, params['time_step'])\n",
    "    elif current_model == 'lstm':\n",
    "        params['time_step'] = trial.suggest_int('time_step', 1, 40)\n",
    "        params['n_layers'] = trial.suggest_int('n_layers', 1, 3)\n",
    "        params['reg_alpha'] = trial.suggest_float('reg_alpha', 1e-6, 1e-2, log=True)\n",
    "        params['reg_lambda'] = trial.suggest_float('reg_lambda', 1e-6, 1e-2, log=True)\n",
    "        for i in range(params['n_layers']):\n",
    "            params['n_units_' + str(i)] = trial.suggest_categorical(\"n_units_{}\".format(i), [16, 32, 64, 128])\n",
    "        params['learning_rate'] = trial.suggest_float('learning_rate', 1e-6, 1e-1, log = True)\n",
    "        params['eta_decay'] = trial.suggest_float('eta_decay', 0.5, 0.9)\n",
    "        #split train-test set\n",
    "        x_train, x_valid, x_test, y_train, y_valid, y_test = custom_data_split(df, params['time_step'])\n",
    "        \n",
    "    #OBJECTIVE\n",
    "    if current_objective == 'mse':\n",
    "        params['loss'] = tf.keras.losses.MeanSquaredError()\n",
    "        params['metric'] = tf.keras.metrics.MeanSquaredError()\n",
    "        params['monitor'] = 'val_mean_squared_error'\n",
    "    elif current_objective =='quantile':\n",
    "        params['loss'] = tfa.losses.PinballLoss(tau = quantile, name = 'quantile')\n",
    "        params['metric'] = tfa.losses.PinballLoss(tau = quantile, name = 'quantile', reduction = tf.keras.losses.Reduction.NONE)\n",
    "        params['monitor'] = 'val_quantile'\n",
    "        \n",
    "    #create model\n",
    "    model = create_model(current_model, params)\n",
    "    \n",
    "    #train model\n",
    "    history = train_model(current_model, model, params, x_train, x_valid, y_train, y_valid, trial)\n",
    "    \n",
    "    #get validation score\n",
    "    valid_score = history.history[params['monitor']][-1]\n",
    "    \n",
    "    #return score to optuna\n",
    "    return valid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "578e0e67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-20 00:17:43,867]\u001b[0m A new study created in memory with name: no-name-ed2eca8c-15bf-4ac3-8e19-8bd5bf2b6155\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:18:18,871]\u001b[0m Trial 0 finished with value: 16.907493591308594 and parameters: {'n_layers': 1, 'reg_alpha': 0.0008179169587463382, 'reg_lambda': 6.410826140753051e-05, 'dropout_rate': 0.13469147625550457, 'n_units_0': 64, 'learning_rate': 0.0024135044592857595, 'eta_decay': 0.5998877876548145}. Best is trial 0 with value: 16.907493591308594.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:18:53,972]\u001b[0m Trial 1 finished with value: 17.42340660095215 and parameters: {'n_layers': 1, 'reg_alpha': 0.00011986210109864836, 'reg_lambda': 1.709287548216866e-05, 'dropout_rate': 0.14862648014077226, 'n_units_0': 128, 'learning_rate': 0.0005927430339979375, 'eta_decay': 0.7501937543811001}. Best is trial 0 with value: 16.907493591308594.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:19:29,794]\u001b[0m Trial 2 finished with value: 17.31326675415039 and parameters: {'n_layers': 1, 'reg_alpha': 0.0007317399061705723, 'reg_lambda': 0.0032065137054509735, 'dropout_rate': 0.15383379241483944, 'n_units_0': 256, 'learning_rate': 0.0011493930829878944, 'eta_decay': 0.7411233851956418}. Best is trial 0 with value: 16.907493591308594.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:19:37,290]\u001b[0m Trial 3 finished with value: 8.388364791870117 and parameters: {'n_layers': 3, 'reg_alpha': 1.1498315203916032e-06, 'reg_lambda': 7.050847939491361e-05, 'dropout_rate': 0.10754258658897081, 'n_units_0': 32, 'n_units_1': 128, 'n_units_2': 256, 'learning_rate': 0.019485683992946156, 'eta_decay': 0.7569374166569475}. Best is trial 3 with value: 8.388364791870117.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:15,830]\u001b[0m Trial 4 finished with value: 41.21693420410156 and parameters: {'n_layers': 3, 'reg_alpha': 0.0004359417952610605, 'reg_lambda': 2.6708968860642575e-05, 'dropout_rate': 0.23856613304904598, 'n_units_0': 32, 'n_units_1': 64, 'n_units_2': 32, 'learning_rate': 4.0556607024446185e-06, 'eta_decay': 0.8741034101225711}. Best is trial 3 with value: 8.388364791870117.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:16,505]\u001b[0m Trial 5 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:17,156]\u001b[0m Trial 6 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:17,660]\u001b[0m Trial 7 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:18,260]\u001b[0m Trial 8 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:18,865]\u001b[0m Trial 9 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:24,243]\u001b[0m Trial 10 finished with value: 9.885069847106934 and parameters: {'n_layers': 3, 'reg_alpha': 1.1700713395452323e-06, 'reg_lambda': 1.1467680429456298e-06, 'dropout_rate': 0.10024135024149135, 'n_units_0': 256, 'n_units_1': 128, 'n_units_2': 128, 'learning_rate': 0.06235804572791926, 'eta_decay': 0.8657896461065805}. Best is trial 3 with value: 8.388364791870117.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:28,170]\u001b[0m Trial 11 finished with value: 10.682808876037598 and parameters: {'n_layers': 3, 'reg_alpha': 1.0034558347373046e-06, 'reg_lambda': 1.3727623472932476e-06, 'dropout_rate': 0.10036186496053563, 'n_units_0': 256, 'n_units_1': 128, 'n_units_2': 128, 'learning_rate': 0.08510184820726444, 'eta_decay': 0.8919073961746625}. Best is trial 3 with value: 8.388364791870117.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:31,110]\u001b[0m Trial 12 finished with value: 10.586023330688477 and parameters: {'n_layers': 2, 'reg_alpha': 1.110453615377284e-06, 'reg_lambda': 0.00024725134423275583, 'dropout_rate': 0.12000351553668295, 'n_units_0': 256, 'n_units_1': 128, 'learning_rate': 0.07110878131904523, 'eta_decay': 0.524145623890604}. Best is trial 3 with value: 8.388364791870117.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:31,807]\u001b[0m Trial 13 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:32,475]\u001b[0m Trial 14 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:33,103]\u001b[0m Trial 15 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:33,772]\u001b[0m Trial 16 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:34,391]\u001b[0m Trial 17 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:40,834]\u001b[0m Trial 18 finished with value: 8.275333404541016 and parameters: {'n_layers': 3, 'reg_alpha': 2.4792877662914264e-05, 'reg_lambda': 0.0001301659379415971, 'dropout_rate': 0.12542958195993387, 'n_units_0': 128, 'n_units_1': 128, 'n_units_2': 128, 'learning_rate': 0.00605958324327278, 'eta_decay': 0.6794457798191982}. Best is trial 18 with value: 8.275333404541016.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:41,504]\u001b[0m Trial 19 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:42,122]\u001b[0m Trial 20 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:42,979]\u001b[0m Trial 21 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:43,671]\u001b[0m Trial 22 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:44,397]\u001b[0m Trial 23 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:45,041]\u001b[0m Trial 24 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:45,665]\u001b[0m Trial 25 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:46,370]\u001b[0m Trial 26 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:47,034]\u001b[0m Trial 27 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:47,767]\u001b[0m Trial 28 pruned. Trial was pruned at epoch 6.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:48,441]\u001b[0m Trial 29 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:49,349]\u001b[0m Trial 30 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:50,311]\u001b[0m Trial 31 pruned. Trial was pruned at epoch 8.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:50,955]\u001b[0m Trial 32 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:51,478]\u001b[0m Trial 33 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:52,128]\u001b[0m Trial 34 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:52,855]\u001b[0m Trial 35 pruned. Trial was pruned at epoch 8.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:53,487]\u001b[0m Trial 36 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:54,464]\u001b[0m Trial 37 pruned. Trial was pruned at epoch 8.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:55,187]\u001b[0m Trial 38 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:55,773]\u001b[0m Trial 39 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:56,666]\u001b[0m Trial 40 pruned. Trial was pruned at epoch 8.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:57,669]\u001b[0m Trial 41 pruned. Trial was pruned at epoch 7.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:58,725]\u001b[0m Trial 42 pruned. Trial was pruned at epoch 8.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:20:59,450]\u001b[0m Trial 43 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:21:04,932]\u001b[0m Trial 44 finished with value: 8.096294403076172 and parameters: {'n_layers': 3, 'reg_alpha': 6.212514791077957e-06, 'reg_lambda': 2.326652392916294e-06, 'dropout_rate': 0.10456554658254263, 'n_units_0': 256, 'n_units_1': 128, 'n_units_2': 32, 'learning_rate': 0.0219363399910911, 'eta_decay': 0.8507050517321648}. Best is trial 44 with value: 8.096294403076172.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:21:05,636]\u001b[0m Trial 45 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:21:06,313]\u001b[0m Trial 46 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:21:07,003]\u001b[0m Trial 47 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:21:07,612]\u001b[0m Trial 48 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:21:08,440]\u001b[0m Trial 49 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:21:08,441]\u001b[0m A new study created in memory with name: no-name-495f4c5d-aef0-4a34-8ca0-865b5096536d\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:21:09,647]\u001b[0m Trial 0 finished with value: 222.8911590576172 and parameters: {'n_layers': 1, 'reg_alpha': 3.305705170248536e-05, 'reg_lambda': 0.0001796222890784113, 'dropout_rate': 0.20266086229961686, 'n_units_0': 32, 'learning_rate': 2.4516151051688454e-06, 'eta_decay': 0.8331947568302414}. Best is trial 0 with value: 222.8911590576172.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:21:19,724]\u001b[0m Trial 1 finished with value: 16.519969940185547 and parameters: {'n_layers': 3, 'reg_alpha': 0.0005665485832448446, 'reg_lambda': 0.0005852153927880444, 'dropout_rate': 0.25733759825524144, 'n_units_0': 128, 'n_units_1': 64, 'n_units_2': 128, 'learning_rate': 0.002410282665112267, 'eta_decay': 0.750028914290908}. Best is trial 1 with value: 16.519969940185547.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-20 00:21:45,363]\u001b[0m Trial 2 finished with value: 21.53697395324707 and parameters: {'n_layers': 3, 'reg_alpha': 0.005272998712870087, 'reg_lambda': 0.0008631549891994171, 'dropout_rate': 0.1910360937987662, 'n_units_0': 64, 'n_units_1': 32, 'n_units_2': 64, 'learning_rate': 0.0004647816363223246, 'eta_decay': 0.7452358321916969}. Best is trial 1 with value: 16.519969940185547.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:22:33,273]\u001b[0m Trial 3 finished with value: 68.61414337158203 and parameters: {'n_layers': 3, 'reg_alpha': 0.0015981222076715345, 'reg_lambda': 1.811304997180709e-06, 'dropout_rate': 0.11479087073895697, 'n_units_0': 256, 'n_units_1': 64, 'n_units_2': 64, 'learning_rate': 5.186324821878977e-06, 'eta_decay': 0.8955121244094748}. Best is trial 1 with value: 16.519969940185547.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:22:53,412]\u001b[0m Trial 4 finished with value: 24.425203323364258 and parameters: {'n_layers': 3, 'reg_alpha': 5.123130260887064e-06, 'reg_lambda': 0.0007683498290857007, 'dropout_rate': 0.2125591066755165, 'n_units_0': 32, 'n_units_1': 256, 'n_units_2': 64, 'learning_rate': 0.00027147543676572566, 'eta_decay': 0.8369176666396345}. Best is trial 1 with value: 16.519969940185547.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:22:53,983]\u001b[0m Trial 5 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:22:54,745]\u001b[0m Trial 6 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:22:59,455]\u001b[0m Trial 7 finished with value: 23.17780303955078 and parameters: {'n_layers': 2, 'reg_alpha': 0.0001351134164610465, 'reg_lambda': 0.003331681945538637, 'dropout_rate': 0.17584380540821976, 'n_units_0': 256, 'n_units_1': 32, 'learning_rate': 0.08272671064776119, 'eta_decay': 0.7489784821928634}. Best is trial 1 with value: 16.519969940185547.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:23:02,664]\u001b[0m Trial 8 pruned. Trial was pruned at epoch 56.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:23:03,285]\u001b[0m Trial 9 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:23:18,920]\u001b[0m Trial 10 finished with value: 16.72471046447754 and parameters: {'n_layers': 2, 'reg_alpha': 0.0003022848185956976, 'reg_lambda': 2.0705849533084998e-05, 'dropout_rate': 0.2815832203016601, 'n_units_0': 128, 'n_units_1': 64, 'learning_rate': 0.002925440835705587, 'eta_decay': 0.6535673184532612}. Best is trial 1 with value: 16.519969940185547.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:23:32,327]\u001b[0m Trial 11 finished with value: 16.015689849853516 and parameters: {'n_layers': 2, 'reg_alpha': 0.0003292624191114745, 'reg_lambda': 2.710160495505777e-05, 'dropout_rate': 0.29060862233655127, 'n_units_0': 128, 'n_units_1': 64, 'learning_rate': 0.004381503682976994, 'eta_decay': 0.6545573090226162}. Best is trial 11 with value: 16.015689849853516.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:23:37,629]\u001b[0m Trial 12 finished with value: 17.941268920898438 and parameters: {'n_layers': 2, 'reg_alpha': 0.0005037984484849244, 'reg_lambda': 2.2739194928676763e-05, 'dropout_rate': 0.2916280107895932, 'n_units_0': 128, 'n_units_1': 64, 'learning_rate': 0.011339864632975014, 'eta_decay': 0.502285686022356}. Best is trial 11 with value: 16.015689849853516.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:23:38,245]\u001b[0m Trial 13 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:23:43,895]\u001b[0m Trial 14 finished with value: 17.782711029052734 and parameters: {'n_layers': 3, 'reg_alpha': 0.001074216810151808, 'reg_lambda': 9.361343308013244e-05, 'dropout_rate': 0.28371412029507376, 'n_units_0': 64, 'n_units_1': 64, 'n_units_2': 32, 'learning_rate': 0.050802928235662216, 'eta_decay': 0.7440307770594314}. Best is trial 11 with value: 16.015689849853516.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:23:44,517]\u001b[0m Trial 15 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:23:45,235]\u001b[0m Trial 16 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:23:45,758]\u001b[0m Trial 17 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:23:46,549]\u001b[0m Trial 18 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:23:47,497]\u001b[0m Trial 19 pruned. Trial was pruned at epoch 8.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:23:48,101]\u001b[0m Trial 20 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:23:48,817]\u001b[0m Trial 21 pruned. Trial was pruned at epoch 5.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:23:49,441]\u001b[0m Trial 22 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:23:50,469]\u001b[0m Trial 23 pruned. Trial was pruned at epoch 11.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:23:50,989]\u001b[0m Trial 24 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:23:59,955]\u001b[0m Trial 25 finished with value: 16.56475067138672 and parameters: {'n_layers': 2, 'reg_alpha': 0.00021275102778238407, 'reg_lambda': 3.726092346187975e-06, 'dropout_rate': 0.22563754490089785, 'n_units_0': 128, 'n_units_1': 128, 'learning_rate': 0.009591783041462614, 'eta_decay': 0.5225709528542586}. Best is trial 11 with value: 16.015689849853516.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:24:00,658]\u001b[0m Trial 26 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:24:01,325]\u001b[0m Trial 27 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:24:02,117]\u001b[0m Trial 28 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:24:02,629]\u001b[0m Trial 29 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:24:03,368]\u001b[0m Trial 30 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:24:04,039]\u001b[0m Trial 31 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:24:04,666]\u001b[0m Trial 32 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:24:05,282]\u001b[0m Trial 33 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:24:06,004]\u001b[0m Trial 34 pruned. Trial was pruned at epoch 5.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:24:06,843]\u001b[0m Trial 35 pruned. Trial was pruned at epoch 5.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:24:07,481]\u001b[0m Trial 36 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:24:08,173]\u001b[0m Trial 37 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:24:12,316]\u001b[0m Trial 38 finished with value: 27.376667022705078 and parameters: {'n_layers': 3, 'reg_alpha': 0.00012742982627686567, 'reg_lambda': 0.00955719742990387, 'dropout_rate': 0.21207335726543824, 'n_units_0': 32, 'n_units_1': 64, 'n_units_2': 128, 'learning_rate': 0.0395513837091372, 'eta_decay': 0.8596075768121714}. Best is trial 11 with value: 16.015689849853516.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:24:13,041]\u001b[0m Trial 39 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:24:13,650]\u001b[0m Trial 40 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:24:16,976]\u001b[0m Trial 41 finished with value: 18.03839683532715 and parameters: {'n_layers': 3, 'reg_alpha': 0.0011863391889861423, 'reg_lambda': 8.904002471616083e-05, 'dropout_rate': 0.28014202718425407, 'n_units_0': 64, 'n_units_1': 64, 'n_units_2': 32, 'learning_rate': 0.05747641089651692, 'eta_decay': 0.7310804152128828}. Best is trial 11 with value: 16.015689849853516.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:24:17,635]\u001b[0m Trial 42 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:24:22,582]\u001b[0m Trial 43 finished with value: 16.58326530456543 and parameters: {'n_layers': 3, 'reg_alpha': 0.0010375901439265804, 'reg_lambda': 1.5879510709459035e-05, 'dropout_rate': 0.26836735845175513, 'n_units_0': 64, 'n_units_1': 64, 'n_units_2': 32, 'learning_rate': 0.04377739036020896, 'eta_decay': 0.705092104198394}. Best is trial 11 with value: 16.015689849853516.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:24:23,267]\u001b[0m Trial 44 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:24:23,949]\u001b[0m Trial 45 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:24:24,757]\u001b[0m Trial 46 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:24:29,450]\u001b[0m Trial 47 finished with value: 21.2869815826416 and parameters: {'n_layers': 3, 'reg_alpha': 0.00015061233726878506, 'reg_lambda': 7.288791118770777e-06, 'dropout_rate': 0.18121672267355388, 'n_units_0': 256, 'n_units_1': 64, 'n_units_2': 256, 'learning_rate': 0.018322151613710487, 'eta_decay': 0.7095290734629012}. Best is trial 11 with value: 16.015689849853516.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:24:30,079]\u001b[0m Trial 48 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:24:33,179]\u001b[0m Trial 49 finished with value: 24.768077850341797 and parameters: {'n_layers': 3, 'reg_alpha': 0.00032306877633789434, 'reg_lambda': 5.750357583165992e-05, 'dropout_rate': 0.2725350249516578, 'n_units_0': 128, 'n_units_1': 64, 'n_units_2': 32, 'learning_rate': 0.059707780846067954, 'eta_decay': 0.808610445310638}. Best is trial 11 with value: 16.015689849853516.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-20 00:24:33,179]\u001b[0m A new study created in memory with name: no-name-aed5149d-df68-4345-9671-b8ed8ed79f35\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:24:35,741]\u001b[0m Trial 0 finished with value: 17.507600784301758 and parameters: {'n_layers': 1, 'reg_alpha': 0.009540825875257007, 'reg_lambda': 0.0032506194452602325, 'dropout_rate': 0.1643982229064812, 'n_units_0': 64, 'learning_rate': 0.05805101377545513, 'eta_decay': 0.832114089325541}. Best is trial 0 with value: 17.507600784301758.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:25:16,706]\u001b[0m Trial 1 finished with value: 331.4285583496094 and parameters: {'n_layers': 2, 'reg_alpha': 0.00016881792596506272, 'reg_lambda': 2.529283985680589e-05, 'dropout_rate': 0.12336741858483537, 'n_units_0': 32, 'n_units_1': 256, 'learning_rate': 4.756401516780025e-06, 'eta_decay': 0.7211538210647754}. Best is trial 0 with value: 17.507600784301758.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:25:25,745]\u001b[0m Trial 2 finished with value: 18.305408477783203 and parameters: {'n_layers': 1, 'reg_alpha': 7.949716958869124e-05, 'reg_lambda': 1.275656457815991e-06, 'dropout_rate': 0.39720083050258215, 'n_units_0': 64, 'learning_rate': 0.003169337573094162, 'eta_decay': 0.530274824132992}. Best is trial 0 with value: 17.507600784301758.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:26:03,734]\u001b[0m Trial 3 finished with value: 16.451892852783203 and parameters: {'n_layers': 1, 'reg_alpha': 2.7804881394635855e-05, 'reg_lambda': 0.006017533818624718, 'dropout_rate': 0.27028722626066026, 'n_units_0': 256, 'learning_rate': 0.0007239596237792218, 'eta_decay': 0.8429458508551629}. Best is trial 3 with value: 16.451892852783203.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:26:20,759]\u001b[0m Trial 4 finished with value: 17.85297393798828 and parameters: {'n_layers': 3, 'reg_alpha': 0.00037100225763652745, 'reg_lambda': 0.0010498642887247903, 'dropout_rate': 0.3726765775021504, 'n_units_0': 128, 'n_units_1': 64, 'n_units_2': 32, 'learning_rate': 0.00012442837692194642, 'eta_decay': 0.693985045862195}. Best is trial 3 with value: 16.451892852783203.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:26:25,922]\u001b[0m Trial 5 finished with value: 16.57951545715332 and parameters: {'n_layers': 1, 'reg_alpha': 0.00326639868871923, 'reg_lambda': 1.2835391847221074e-06, 'dropout_rate': 0.12195673758874281, 'n_units_0': 256, 'learning_rate': 0.016041840005945992, 'eta_decay': 0.5244691486966753}. Best is trial 3 with value: 16.451892852783203.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:26:26,613]\u001b[0m Trial 6 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:26:34,060]\u001b[0m Trial 7 finished with value: 17.351640701293945 and parameters: {'n_layers': 3, 'reg_alpha': 0.0024211087622816817, 'reg_lambda': 2.474423533405786e-05, 'dropout_rate': 0.22949431757228525, 'n_units_0': 64, 'n_units_1': 128, 'n_units_2': 32, 'learning_rate': 0.0007377936931337304, 'eta_decay': 0.8206168628824186}. Best is trial 3 with value: 16.451892852783203.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:26:58,757]\u001b[0m Trial 8 finished with value: 9.194951057434082 and parameters: {'n_layers': 2, 'reg_alpha': 8.111842828761397e-06, 'reg_lambda': 8.485504099805394e-06, 'dropout_rate': 0.2635141652611792, 'n_units_0': 256, 'n_units_1': 256, 'learning_rate': 0.0008705444763254024, 'eta_decay': 0.712438951614092}. Best is trial 8 with value: 9.194951057434082.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:26:59,397]\u001b[0m Trial 9 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:27:00,139]\u001b[0m Trial 10 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:27:15,044]\u001b[0m Trial 11 finished with value: 16.13532257080078 and parameters: {'n_layers': 2, 'reg_alpha': 1.839161745976215e-05, 'reg_lambda': 0.009951387759477914, 'dropout_rate': 0.28136808311086436, 'n_units_0': 256, 'n_units_1': 256, 'learning_rate': 0.001204434262934177, 'eta_decay': 0.8991987603806294}. Best is trial 8 with value: 9.194951057434082.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:27:21,698]\u001b[0m Trial 12 finished with value: 15.929703712463379 and parameters: {'n_layers': 2, 'reg_alpha': 6.732740055974485e-06, 'reg_lambda': 0.00028166845660905065, 'dropout_rate': 0.29882083161953304, 'n_units_0': 256, 'n_units_1': 256, 'learning_rate': 0.005088331524490674, 'eta_decay': 0.7597618843742955}. Best is trial 8 with value: 9.194951057434082.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:27:27,249]\u001b[0m Trial 13 finished with value: 16.588890075683594 and parameters: {'n_layers': 2, 'reg_alpha': 1.4933728833260738e-06, 'reg_lambda': 0.00020704336252210057, 'dropout_rate': 0.32288777545243263, 'n_units_0': 256, 'n_units_1': 256, 'learning_rate': 0.006993076497244016, 'eta_decay': 0.7490068828909394}. Best is trial 8 with value: 9.194951057434082.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:27:28,017]\u001b[0m Trial 14 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:27:30,446]\u001b[0m Trial 15 pruned. Trial was pruned at epoch 29.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:27:31,091]\u001b[0m Trial 16 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:27:37,875]\u001b[0m Trial 17 finished with value: 16.051664352416992 and parameters: {'n_layers': 1, 'reg_alpha': 2.9651669628413072e-06, 'reg_lambda': 8.275784065427955e-05, 'dropout_rate': 0.19217106346886523, 'n_units_0': 128, 'learning_rate': 0.01222837329118375, 'eta_decay': 0.7340257054617837}. Best is trial 8 with value: 9.194951057434082.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:27:38,557]\u001b[0m Trial 18 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:27:39,502]\u001b[0m Trial 19 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:27:40,271]\u001b[0m Trial 20 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:27:43,061]\u001b[0m Trial 21 pruned. Trial was pruned at epoch 51.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:27:43,615]\u001b[0m Trial 22 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:27:46,493]\u001b[0m Trial 23 pruned. Trial was pruned at epoch 54.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:27:47,018]\u001b[0m Trial 24 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:27:48,929]\u001b[0m Trial 25 pruned. Trial was pruned at epoch 27.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:27:51,556]\u001b[0m Trial 26 pruned. Trial was pruned at epoch 41.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:27:53,805]\u001b[0m Trial 27 pruned. Trial was pruned at epoch 32.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:28:09,803]\u001b[0m Trial 28 finished with value: 16.031951904296875 and parameters: {'n_layers': 1, 'reg_alpha': 1.0221912294858248e-06, 'reg_lambda': 6.180262424972574e-05, 'dropout_rate': 0.2868501125232167, 'n_units_0': 256, 'learning_rate': 0.006211901268589803, 'eta_decay': 0.6807376116373947}. Best is trial 8 with value: 9.194951057434082.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:28:10,341]\u001b[0m Trial 29 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:28:11,095]\u001b[0m Trial 30 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:28:19,390]\u001b[0m Trial 31 finished with value: 15.997737884521484 and parameters: {'n_layers': 1, 'reg_alpha': 3.4343110562440685e-06, 'reg_lambda': 6.967496890466322e-05, 'dropout_rate': 0.2651879009720766, 'n_units_0': 256, 'learning_rate': 0.007461865448911182, 'eta_decay': 0.7376098088306766}. Best is trial 8 with value: 9.194951057434082.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:28:19,921]\u001b[0m Trial 32 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:28:24,194]\u001b[0m Trial 33 finished with value: 15.6055326461792 and parameters: {'n_layers': 1, 'reg_alpha': 0.00012504788800893931, 'reg_lambda': 0.00015390155288410912, 'dropout_rate': 0.3067054037807648, 'n_units_0': 256, 'learning_rate': 0.04309379088419775, 'eta_decay': 0.7023608248959828}. Best is trial 8 with value: 9.194951057434082.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:28:27,694]\u001b[0m Trial 34 finished with value: 16.404781341552734 and parameters: {'n_layers': 1, 'reg_alpha': 0.000123144373847498, 'reg_lambda': 0.0003504849074178887, 'dropout_rate': 0.3640032783917165, 'n_units_0': 256, 'learning_rate': 0.04435757520199197, 'eta_decay': 0.7188265137682867}. Best is trial 8 with value: 9.194951057434082.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:28:28,649]\u001b[0m Trial 35 pruned. Trial was pruned at epoch 13.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:28:29,187]\u001b[0m Trial 36 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:28:29,919]\u001b[0m Trial 37 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:28:32,904]\u001b[0m Trial 38 finished with value: 17.175172805786133 and parameters: {'n_layers': 2, 'reg_alpha': 0.0007422648338959055, 'reg_lambda': 0.0001394892670371696, 'dropout_rate': 0.3560189698932873, 'n_units_0': 256, 'n_units_1': 128, 'learning_rate': 0.009874022527513572, 'eta_decay': 0.7017164196309499}. Best is trial 8 with value: 9.194951057434082.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-20 00:28:33,842]\u001b[0m Trial 39 pruned. Trial was pruned at epoch 9.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:28:34,650]\u001b[0m Trial 40 pruned. Trial was pruned at epoch 5.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:28:35,188]\u001b[0m Trial 41 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:28:35,728]\u001b[0m Trial 42 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:28:36,264]\u001b[0m Trial 43 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:28:36,800]\u001b[0m Trial 44 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:28:37,436]\u001b[0m Trial 45 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:28:37,971]\u001b[0m Trial 46 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:28:38,511]\u001b[0m Trial 47 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:28:39,274]\u001b[0m Trial 48 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:28:39,893]\u001b[0m Trial 49 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:28:39,893]\u001b[0m A new study created in memory with name: no-name-8281e6cf-b8c2-4e19-9397-538b1203bac5\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:31:57,280]\u001b[0m Trial 0 finished with value: 41.37488555908203 and parameters: {'time_step': 15, 'n_layers': 2, 'reg_alpha': 5.259428119928567e-06, 'reg_lambda': 1.236385131345823e-05, 'n_units_0': 64, 'n_units_1': 64, 'learning_rate': 3.96050342617686e-05, 'eta_decay': 0.8961267668920048}. Best is trial 0 with value: 41.37488555908203.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:32:10,105]\u001b[0m Trial 1 finished with value: 22.381057739257812 and parameters: {'time_step': 1, 'n_layers': 1, 'reg_alpha': 0.005178252095227008, 'reg_lambda': 4.339094492266085e-06, 'n_units_0': 16, 'learning_rate': 0.0006772066329727389, 'eta_decay': 0.500477960251484}. Best is trial 1 with value: 22.381057739257812.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:33:05,594]\u001b[0m Trial 2 finished with value: 44.4751091003418 and parameters: {'time_step': 5, 'n_layers': 3, 'reg_alpha': 0.00893845841331596, 'reg_lambda': 5.758583671406392e-06, 'n_units_0': 32, 'n_units_1': 16, 'n_units_2': 64, 'learning_rate': 1.8141100731985989e-06, 'eta_decay': 0.6125410086942527}. Best is trial 1 with value: 22.381057739257812.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:36:29,983]\u001b[0m Trial 3 finished with value: 5.093223571777344 and parameters: {'time_step': 35, 'n_layers': 3, 'reg_alpha': 1.5886267134235085e-05, 'reg_lambda': 1.5445485618503398e-06, 'n_units_0': 16, 'n_units_1': 64, 'n_units_2': 32, 'learning_rate': 0.0018911125818429114, 'eta_decay': 0.738985041330926}. Best is trial 3 with value: 5.093223571777344.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:38:24,236]\u001b[0m Trial 4 finished with value: 28.062744140625 and parameters: {'time_step': 34, 'n_layers': 2, 'reg_alpha': 3.470857463316276e-05, 'reg_lambda': 3.4522911990876283e-05, 'n_units_0': 64, 'n_units_1': 128, 'learning_rate': 0.042472579641818116, 'eta_decay': 0.7972367479112605}. Best is trial 3 with value: 5.093223571777344.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:38:56,635]\u001b[0m Trial 5 finished with value: 9.257301330566406 and parameters: {'time_step': 21, 'n_layers': 1, 'reg_alpha': 0.00018158109673089998, 'reg_lambda': 0.0064147911623408625, 'n_units_0': 32, 'learning_rate': 0.002097371452289323, 'eta_decay': 0.7091073471951405}. Best is trial 3 with value: 5.093223571777344.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:39:18,483]\u001b[0m Trial 6 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:39:29,695]\u001b[0m Trial 7 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:39:56,556]\u001b[0m Trial 8 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:40:14,998]\u001b[0m Trial 9 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:43:28,400]\u001b[0m Trial 10 finished with value: 5.007742404937744 and parameters: {'time_step': 39, 'n_layers': 3, 'reg_alpha': 1.5759394308878862e-05, 'reg_lambda': 1.393516019160741e-06, 'n_units_0': 16, 'n_units_1': 32, 'n_units_2': 32, 'learning_rate': 0.006509824301581621, 'eta_decay': 0.7418381989622185}. Best is trial 10 with value: 5.007742404937744.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:45:16,173]\u001b[0m Trial 11 finished with value: 5.496057033538818 and parameters: {'time_step': 40, 'n_layers': 3, 'reg_alpha': 1.9400683033572014e-05, 'reg_lambda': 1.1826609168199997e-06, 'n_units_0': 16, 'n_units_1': 32, 'n_units_2': 32, 'learning_rate': 0.005408109000278363, 'eta_decay': 0.7551810226161727}. Best is trial 10 with value: 5.007742404937744.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:47:29,324]\u001b[0m Trial 12 finished with value: 6.483363151550293 and parameters: {'time_step': 40, 'n_layers': 3, 'reg_alpha': 1.1755611210654919e-05, 'reg_lambda': 2.9152113875165884e-06, 'n_units_0': 16, 'n_units_1': 32, 'n_units_2': 32, 'learning_rate': 0.00938364327251401, 'eta_decay': 0.6349680698949022}. Best is trial 10 with value: 5.007742404937744.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:47:43,575]\u001b[0m Trial 13 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:51:00,184]\u001b[0m Trial 14 finished with value: 6.0438079833984375 and parameters: {'time_step': 29, 'n_layers': 3, 'reg_alpha': 6.650683837431067e-05, 'reg_lambda': 0.00022572230499019364, 'n_units_0': 16, 'n_units_1': 64, 'n_units_2': 32, 'learning_rate': 0.00969669356276241, 'eta_decay': 0.5788611041637522}. Best is trial 10 with value: 5.007742404937744.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:51:19,497]\u001b[0m Trial 15 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:51:39,913]\u001b[0m Trial 16 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:51:54,941]\u001b[0m Trial 17 finished with value: 11.049914360046387 and parameters: {'time_step': 27, 'n_layers': 1, 'reg_alpha': 2.2286553496965556e-05, 'reg_lambda': 1.5296175326351728e-05, 'n_units_0': 16, 'learning_rate': 0.020817869703580055, 'eta_decay': 0.7968654888893913}. Best is trial 10 with value: 5.007742404937744.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:52:07,712]\u001b[0m Trial 18 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:54:22,361]\u001b[0m Trial 19 pruned. Trial was pruned at epoch 30.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:54:43,149]\u001b[0m Trial 20 pruned. Trial was pruned at epoch 22.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:57:59,213]\u001b[0m Trial 21 finished with value: 5.414173126220703 and parameters: {'time_step': 40, 'n_layers': 3, 'reg_alpha': 1.5861211050651378e-05, 'reg_lambda': 1.1412154040679091e-06, 'n_units_0': 16, 'n_units_1': 32, 'n_units_2': 32, 'learning_rate': 0.005397554655845217, 'eta_decay': 0.7515966306686467}. Best is trial 10 with value: 5.007742404937744.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:58:13,700]\u001b[0m Trial 22 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 00:58:28,222]\u001b[0m Trial 23 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:01:19,819]\u001b[0m Trial 24 finished with value: 10.197474479675293 and parameters: {'time_step': 36, 'n_layers': 3, 'reg_alpha': 9.37933353673483e-06, 'reg_lambda': 3.074227110101073e-06, 'n_units_0': 16, 'n_units_1': 32, 'n_units_2': 128, 'learning_rate': 0.0077655976262077375, 'eta_decay': 0.6897134620575707}. Best is trial 10 with value: 5.007742404937744.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:02:21,395]\u001b[0m Trial 25 pruned. Trial was pruned at epoch 25.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:02:35,606]\u001b[0m Trial 26 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:02:46,580]\u001b[0m Trial 27 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:03:06,870]\u001b[0m Trial 28 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:03:21,956]\u001b[0m Trial 29 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:03:41,948]\u001b[0m Trial 30 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:05:24,390]\u001b[0m Trial 31 finished with value: 6.482555389404297 and parameters: {'time_step': 40, 'n_layers': 3, 'reg_alpha': 1.83344935136102e-05, 'reg_lambda': 1.0867133375868637e-06, 'n_units_0': 16, 'n_units_1': 32, 'n_units_2': 32, 'learning_rate': 0.006405933383475395, 'eta_decay': 0.7501525165494387}. Best is trial 10 with value: 5.007742404937744.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:05:40,138]\u001b[0m Trial 32 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:05:54,875]\u001b[0m Trial 33 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:07:21,797]\u001b[0m Trial 34 finished with value: 6.892747402191162 and parameters: {'time_step': 35, 'n_layers': 3, 'reg_alpha': 2.8353567180612333e-05, 'reg_lambda': 1.8587225854631358e-06, 'n_units_0': 16, 'n_units_1': 32, 'n_units_2': 32, 'learning_rate': 0.014163290731676523, 'eta_decay': 0.5172396800638296}. Best is trial 10 with value: 5.007742404937744.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-20 01:07:28,836]\u001b[0m Trial 35 pruned. Trial was pruned at epoch 32.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:07:37,871]\u001b[0m Trial 36 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:08:01,168]\u001b[0m Trial 37 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:08:14,316]\u001b[0m Trial 38 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:08:41,953]\u001b[0m Trial 39 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:09:01,176]\u001b[0m Trial 40 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:09:26,206]\u001b[0m Trial 41 pruned. Trial was pruned at epoch 6.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:09:47,714]\u001b[0m Trial 42 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:10:07,053]\u001b[0m Trial 43 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:10:23,767]\u001b[0m Trial 44 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:11:33,282]\u001b[0m Trial 45 pruned. Trial was pruned at epoch 5.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:11:51,533]\u001b[0m Trial 46 pruned. Trial was pruned at epoch 9.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:12:05,695]\u001b[0m Trial 47 pruned. Trial was pruned at epoch 6.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:13:03,424]\u001b[0m Trial 48 pruned. Trial was pruned at epoch 25.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:13:17,939]\u001b[0m Trial 49 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:13:17,939]\u001b[0m A new study created in memory with name: no-name-d6254540-cfe8-4a15-826a-8abf1fb6a2e5\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:13:47,225]\u001b[0m Trial 0 finished with value: 28.66828155517578 and parameters: {'time_step': 28, 'n_layers': 1, 'reg_alpha': 0.007885863361913588, 'reg_lambda': 0.0005974061512405509, 'n_units_0': 16, 'learning_rate': 0.005066495838596229, 'eta_decay': 0.8462232180757099}. Best is trial 0 with value: 28.66828155517578.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:15:32,895]\u001b[0m Trial 1 finished with value: 58.53749084472656 and parameters: {'time_step': 29, 'n_layers': 2, 'reg_alpha': 4.738009327943793e-06, 'reg_lambda': 0.0021543495083740723, 'n_units_0': 32, 'n_units_1': 16, 'learning_rate': 0.00036897786843973564, 'eta_decay': 0.567640420918896}. Best is trial 0 with value: 28.66828155517578.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:18:30,595]\u001b[0m Trial 2 finished with value: 17.07394027709961 and parameters: {'time_step': 20, 'n_layers': 2, 'reg_alpha': 0.0010869097413095485, 'reg_lambda': 0.0067339907486725446, 'n_units_0': 32, 'n_units_1': 64, 'learning_rate': 0.0038193996327849826, 'eta_decay': 0.7048466178996166}. Best is trial 2 with value: 17.07394027709961.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:24:14,652]\u001b[0m Trial 3 finished with value: 221.9331817626953 and parameters: {'time_step': 33, 'n_layers': 1, 'reg_alpha': 2.9124866033072784e-06, 'reg_lambda': 2.3191105252830986e-05, 'n_units_0': 128, 'learning_rate': 2.1963350563124346e-06, 'eta_decay': 0.5503408239655897}. Best is trial 2 with value: 17.07394027709961.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:25:54,201]\u001b[0m Trial 4 finished with value: 167.5933837890625 and parameters: {'time_step': 9, 'n_layers': 2, 'reg_alpha': 1.1438621706021919e-06, 'reg_lambda': 0.00045460981908149135, 'n_units_0': 64, 'n_units_1': 32, 'learning_rate': 0.00013443666398968177, 'eta_decay': 0.8966336876809062}. Best is trial 2 with value: 17.07394027709961.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:26:02,302]\u001b[0m Trial 5 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:26:33,816]\u001b[0m Trial 6 finished with value: 46.031044006347656 and parameters: {'time_step': 29, 'n_layers': 1, 'reg_alpha': 0.0005913307285156839, 'reg_lambda': 2.017233260846725e-06, 'n_units_0': 32, 'learning_rate': 0.07472582109160422, 'eta_decay': 0.8348283302812316}. Best is trial 2 with value: 17.07394027709961.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:26:54,378]\u001b[0m Trial 7 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:27:08,750]\u001b[0m Trial 8 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:27:27,642]\u001b[0m Trial 9 finished with value: 58.01899719238281 and parameters: {'time_step': 5, 'n_layers': 3, 'reg_alpha': 0.008509011228483415, 'reg_lambda': 7.669873395751777e-05, 'n_units_0': 16, 'n_units_1': 16, 'n_units_2': 32, 'learning_rate': 0.046303367003627194, 'eta_decay': 0.8327288935237573}. Best is trial 2 with value: 17.07394027709961.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:27:43,459]\u001b[0m Trial 10 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:28:07,760]\u001b[0m Trial 11 pruned. Trial was pruned at epoch 7.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:28:23,222]\u001b[0m Trial 12 finished with value: 45.9180793762207 and parameters: {'time_step': 15, 'n_layers': 1, 'reg_alpha': 0.0013512815936140162, 'reg_lambda': 0.008457975127250106, 'n_units_0': 32, 'learning_rate': 0.006752448951150308, 'eta_decay': 0.6439067954978086}. Best is trial 2 with value: 17.07394027709961.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:28:39,359]\u001b[0m Trial 13 finished with value: 39.10251235961914 and parameters: {'time_step': 24, 'n_layers': 1, 'reg_alpha': 0.001695922019125732, 'reg_lambda': 0.0005045813138312348, 'n_units_0': 16, 'learning_rate': 0.008793062909114184, 'eta_decay': 0.8968575208976518}. Best is trial 2 with value: 17.07394027709961.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:28:54,502]\u001b[0m Trial 14 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:29:00,580]\u001b[0m Trial 15 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:34:50,562]\u001b[0m Trial 16 finished with value: 18.454204559326172 and parameters: {'time_step': 13, 'n_layers': 3, 'reg_alpha': 0.0002796865775341984, 'reg_lambda': 0.00012402946832711938, 'n_units_0': 64, 'n_units_1': 64, 'n_units_2': 128, 'learning_rate': 0.019231150622849642, 'eta_decay': 0.7967851678827533}. Best is trial 2 with value: 17.07394027709961.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:36:10,055]\u001b[0m Trial 17 finished with value: 24.57725715637207 and parameters: {'time_step': 3, 'n_layers': 3, 'reg_alpha': 2.2252255360562197e-05, 'reg_lambda': 8.782337847082829e-05, 'n_units_0': 64, 'n_units_1': 64, 'n_units_2': 128, 'learning_rate': 0.024048265154918733, 'eta_decay': 0.7634766563870099}. Best is trial 2 with value: 17.07394027709961.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:40:59,815]\u001b[0m Trial 18 finished with value: 23.565677642822266 and parameters: {'time_step': 13, 'n_layers': 3, 'reg_alpha': 0.0002428627067829918, 'reg_lambda': 2.5077647426518416e-05, 'n_units_0': 64, 'n_units_1': 64, 'n_units_2': 128, 'learning_rate': 0.024393181746870433, 'eta_decay': 0.5074377356474579}. Best is trial 2 with value: 17.07394027709961.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:41:09,759]\u001b[0m Trial 19 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:41:28,949]\u001b[0m Trial 20 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:42:22,948]\u001b[0m Trial 21 pruned. Trial was pruned at epoch 20.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:44:02,593]\u001b[0m Trial 22 pruned. Trial was pruned at epoch 20.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:45:02,660]\u001b[0m Trial 23 pruned. Trial was pruned at epoch 20.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:45:12,999]\u001b[0m Trial 24 pruned. Trial was pruned at epoch 5.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:46:37,633]\u001b[0m Trial 25 finished with value: 31.97014045715332 and parameters: {'time_step': 18, 'n_layers': 3, 'reg_alpha': 0.0011596006817261864, 'reg_lambda': 3.923048473904778e-06, 'n_units_0': 32, 'n_units_1': 64, 'n_units_2': 16, 'learning_rate': 0.01579042037964927, 'eta_decay': 0.5045444832008302}. Best is trial 2 with value: 17.07394027709961.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:47:36,575]\u001b[0m Trial 26 pruned. Trial was pruned at epoch 67.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:47:54,089]\u001b[0m Trial 27 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:48:14,432]\u001b[0m Trial 28 pruned. Trial was pruned at epoch 8.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:48:37,195]\u001b[0m Trial 29 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:48:49,391]\u001b[0m Trial 30 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:49:12,289]\u001b[0m Trial 31 finished with value: 22.331172943115234 and parameters: {'time_step': 1, 'n_layers': 3, 'reg_alpha': 2.1235817901447635e-05, 'reg_lambda': 6.536909641133535e-05, 'n_units_0': 64, 'n_units_1': 64, 'n_units_2': 128, 'learning_rate': 0.03853103433247965, 'eta_decay': 0.7733535479252633}. Best is trial 2 with value: 17.07394027709961.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:50:21,790]\u001b[0m Trial 32 pruned. Trial was pruned at epoch 67.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-20 01:51:04,030]\u001b[0m Trial 33 finished with value: 27.834400177001953 and parameters: {'time_step': 6, 'n_layers': 3, 'reg_alpha': 5.560574120581462e-05, 'reg_lambda': 0.00021064345096735815, 'n_units_0': 64, 'n_units_1': 64, 'n_units_2': 128, 'learning_rate': 0.028727757128373166, 'eta_decay': 0.7037942141012505}. Best is trial 2 with value: 17.07394027709961.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:51:45,831]\u001b[0m Trial 34 pruned. Trial was pruned at epoch 5.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:51:53,431]\u001b[0m Trial 35 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:52:51,425]\u001b[0m Trial 36 pruned. Trial was pruned at epoch 12.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:53:08,462]\u001b[0m Trial 37 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:53:27,020]\u001b[0m Trial 38 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:53:52,537]\u001b[0m Trial 39 pruned. Trial was pruned at epoch 12.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:54:04,371]\u001b[0m Trial 40 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:54:48,913]\u001b[0m Trial 41 finished with value: 22.54808235168457 and parameters: {'time_step': 3, 'n_layers': 3, 'reg_alpha': 1.8206055214302956e-05, 'reg_lambda': 0.0001018721494275896, 'n_units_0': 64, 'n_units_1': 64, 'n_units_2': 128, 'learning_rate': 0.026047650743659338, 'eta_decay': 0.7653556586137034}. Best is trial 2 with value: 17.07394027709961.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:55:40,819]\u001b[0m Trial 42 finished with value: 20.361387252807617 and parameters: {'time_step': 3, 'n_layers': 3, 'reg_alpha': 1.1842489580665269e-05, 'reg_lambda': 0.00013075087924403398, 'n_units_0': 64, 'n_units_1': 64, 'n_units_2': 128, 'learning_rate': 0.030387057035214497, 'eta_decay': 0.7651558467084569}. Best is trial 2 with value: 17.07394027709961.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:56:12,468]\u001b[0m Trial 43 finished with value: 22.09421730041504 and parameters: {'time_step': 3, 'n_layers': 3, 'reg_alpha': 1.0354141348497122e-05, 'reg_lambda': 0.00012848302861291107, 'n_units_0': 64, 'n_units_1': 64, 'n_units_2': 128, 'learning_rate': 0.05260255613083523, 'eta_decay': 0.7643538896072556}. Best is trial 2 with value: 17.07394027709961.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:57:03,688]\u001b[0m Trial 44 finished with value: 20.3154296875 and parameters: {'time_step': 3, 'n_layers': 3, 'reg_alpha': 3.41726393234598e-06, 'reg_lambda': 0.00028112789536250467, 'n_units_0': 64, 'n_units_1': 64, 'n_units_2': 128, 'learning_rate': 0.06681107446435738, 'eta_decay': 0.7821515025851501}. Best is trial 2 with value: 17.07394027709961.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:57:30,330]\u001b[0m Trial 45 finished with value: 25.30797576904297 and parameters: {'time_step': 6, 'n_layers': 3, 'reg_alpha': 3.113179635035659e-06, 'reg_lambda': 0.0003384029335408091, 'n_units_0': 16, 'n_units_1': 32, 'n_units_2': 32, 'learning_rate': 0.055257915779249066, 'eta_decay': 0.8107638900019738}. Best is trial 2 with value: 17.07394027709961.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:57:42,753]\u001b[0m Trial 46 finished with value: 30.792753219604492 and parameters: {'time_step': 3, 'n_layers': 2, 'reg_alpha': 4.465729955489829e-06, 'reg_lambda': 0.0009221339109693629, 'n_units_0': 32, 'n_units_1': 16, 'learning_rate': 0.09725311796623427, 'eta_decay': 0.7921468579605858}. Best is trial 2 with value: 17.07394027709961.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:57:56,976]\u001b[0m Trial 47 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:58:38,527]\u001b[0m Trial 48 finished with value: 28.787874221801758 and parameters: {'time_step': 4, 'n_layers': 3, 'reg_alpha': 1.869873793781186e-06, 'reg_lambda': 0.00036408296404611643, 'n_units_0': 32, 'n_units_1': 64, 'n_units_2': 128, 'learning_rate': 0.06122561944678905, 'eta_decay': 0.8286436716476234}. Best is trial 2 with value: 17.07394027709961.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:58:47,442]\u001b[0m Trial 49 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:58:47,443]\u001b[0m A new study created in memory with name: no-name-fbef6edf-b5fb-4d1e-a216-696ccc6ce06e\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:59:08,499]\u001b[0m Trial 0 finished with value: 15.684417724609375 and parameters: {'time_step': 8, 'n_layers': 1, 'reg_alpha': 0.0015164699337586996, 'reg_lambda': 0.00012723273535422463, 'n_units_0': 16, 'learning_rate': 0.0015962471672669508, 'eta_decay': 0.7226277310913103}. Best is trial 0 with value: 15.684417724609375.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 01:59:37,095]\u001b[0m Trial 1 finished with value: 26.610605239868164 and parameters: {'time_step': 15, 'n_layers': 1, 'reg_alpha': 6.3887373200626705e-06, 'reg_lambda': 3.1037423636344493e-06, 'n_units_0': 16, 'learning_rate': 0.000494290848790759, 'eta_decay': 0.7461567230671582}. Best is trial 0 with value: 15.684417724609375.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:01:18,085]\u001b[0m Trial 2 finished with value: 17.955615997314453 and parameters: {'time_step': 15, 'n_layers': 3, 'reg_alpha': 5.1678398907521275e-06, 'reg_lambda': 0.000156877792282056, 'n_units_0': 16, 'n_units_1': 32, 'n_units_2': 64, 'learning_rate': 0.03559444321449437, 'eta_decay': 0.6606976472507772}. Best is trial 0 with value: 15.684417724609375.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:06:45,642]\u001b[0m Trial 3 finished with value: 81.7140884399414 and parameters: {'time_step': 33, 'n_layers': 1, 'reg_alpha': 0.0014510031042429723, 'reg_lambda': 6.784842795429043e-05, 'n_units_0': 128, 'learning_rate': 0.0001099298365880932, 'eta_decay': 0.5324546690002513}. Best is trial 0 with value: 15.684417724609375.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:09:06,096]\u001b[0m Trial 4 finished with value: 21.475872039794922 and parameters: {'time_step': 11, 'n_layers': 3, 'reg_alpha': 0.005995075360694026, 'reg_lambda': 7.611818312534648e-06, 'n_units_0': 128, 'n_units_1': 128, 'n_units_2': 128, 'learning_rate': 0.09727116943893697, 'eta_decay': 0.72838994679192}. Best is trial 0 with value: 15.684417724609375.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:09:26,863]\u001b[0m Trial 5 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:09:43,997]\u001b[0m Trial 6 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:09:49,660]\u001b[0m Trial 7 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:09:55,509]\u001b[0m Trial 8 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:10:21,454]\u001b[0m Trial 9 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:12:37,470]\u001b[0m Trial 10 finished with value: 12.640029907226562 and parameters: {'time_step': 39, 'n_layers': 2, 'reg_alpha': 0.0003653134171940562, 'reg_lambda': 0.004060556499966252, 'n_units_0': 32, 'n_units_1': 16, 'learning_rate': 0.005304690755424027, 'eta_decay': 0.8729298833083675}. Best is trial 10 with value: 12.640029907226562.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:15:17,326]\u001b[0m Trial 11 finished with value: 6.860470294952393 and parameters: {'time_step': 40, 'n_layers': 2, 'reg_alpha': 0.0003601058530209024, 'reg_lambda': 0.005484440496672286, 'n_units_0': 32, 'n_units_1': 16, 'learning_rate': 0.007065686569345066, 'eta_decay': 0.8822909097043207}. Best is trial 11 with value: 6.860470294952393.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:17:31,711]\u001b[0m Trial 12 finished with value: 6.740644931793213 and parameters: {'time_step': 40, 'n_layers': 2, 'reg_alpha': 0.00015762146606583497, 'reg_lambda': 0.006930834699777115, 'n_units_0': 32, 'n_units_1': 16, 'learning_rate': 0.007192945554604877, 'eta_decay': 0.892035486091188}. Best is trial 12 with value: 6.740644931793213.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:19:35,632]\u001b[0m Trial 13 finished with value: 8.153432846069336 and parameters: {'time_step': 39, 'n_layers': 2, 'reg_alpha': 6.643621619723207e-05, 'reg_lambda': 0.008092428209506279, 'n_units_0': 32, 'n_units_1': 16, 'learning_rate': 0.010331151145491293, 'eta_decay': 0.8831560993819213}. Best is trial 12 with value: 6.740644931793213.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:19:50,500]\u001b[0m Trial 14 pruned. Trial was pruned at epoch 10.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:20:00,785]\u001b[0m Trial 15 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:20:40,677]\u001b[0m Trial 16 finished with value: 16.12380027770996 and parameters: {'time_step': 26, 'n_layers': 2, 'reg_alpha': 0.0005501518613914285, 'reg_lambda': 0.0008617950852929904, 'n_units_0': 32, 'n_units_1': 16, 'learning_rate': 0.04556801889344259, 'eta_decay': 0.8214611678016794}. Best is trial 12 with value: 6.740644931793213.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-20 02:21:33,372]\u001b[0m Trial 17 pruned. Trial was pruned at epoch 39.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:21:55,664]\u001b[0m Trial 18 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:22:05,586]\u001b[0m Trial 19 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:24:49,552]\u001b[0m Trial 20 finished with value: 19.065217971801758 and parameters: {'time_step': 36, 'n_layers': 2, 'reg_alpha': 0.00015786370688207442, 'reg_lambda': 0.0027303346345052193, 'n_units_0': 64, 'n_units_1': 16, 'learning_rate': 0.027161892442890866, 'eta_decay': 0.8421787186532839}. Best is trial 12 with value: 6.740644931793213.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:25:00,496]\u001b[0m Trial 21 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:25:12,831]\u001b[0m Trial 22 pruned. Trial was pruned at epoch 6.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:25:24,120]\u001b[0m Trial 23 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:25:35,085]\u001b[0m Trial 24 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:26:38,927]\u001b[0m Trial 25 finished with value: 6.290726184844971 and parameters: {'time_step': 29, 'n_layers': 1, 'reg_alpha': 9.860801384060622e-05, 'reg_lambda': 1.0728431920946264e-06, 'n_units_0': 32, 'learning_rate': 0.015242410037401302, 'eta_decay': 0.77582813121938}. Best is trial 25 with value: 6.290726184844971.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:27:04,427]\u001b[0m Trial 26 finished with value: 15.142860412597656 and parameters: {'time_step': 30, 'n_layers': 1, 'reg_alpha': 0.0016660945685095501, 'reg_lambda': 1.369985085319315e-06, 'n_units_0': 32, 'learning_rate': 0.087433402759896, 'eta_decay': 0.7809033987765271}. Best is trial 25 with value: 6.290726184844971.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:27:43,634]\u001b[0m Trial 27 finished with value: 12.997081756591797 and parameters: {'time_step': 24, 'n_layers': 1, 'reg_alpha': 0.0001282769185238443, 'reg_lambda': 2.5775536922562715e-05, 'n_units_0': 32, 'learning_rate': 0.021875432800713884, 'eta_decay': 0.8346128552289156}. Best is trial 25 with value: 6.290726184844971.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:27:49,573]\u001b[0m Trial 28 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:28:01,763]\u001b[0m Trial 29 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:29:04,634]\u001b[0m Trial 30 pruned. Trial was pruned at epoch 14.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:29:18,455]\u001b[0m Trial 31 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:29:26,286]\u001b[0m Trial 32 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:29:36,136]\u001b[0m Trial 33 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:29:45,775]\u001b[0m Trial 34 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:30:26,765]\u001b[0m Trial 35 pruned. Trial was pruned at epoch 20.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:30:34,000]\u001b[0m Trial 36 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:30:54,351]\u001b[0m Trial 37 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:31:07,166]\u001b[0m Trial 38 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:31:46,361]\u001b[0m Trial 39 pruned. Trial was pruned at epoch 62.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:32:12,826]\u001b[0m Trial 40 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:32:23,984]\u001b[0m Trial 41 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:32:51,558]\u001b[0m Trial 42 pruned. Trial was pruned at epoch 62.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:33:02,579]\u001b[0m Trial 43 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:33:50,146]\u001b[0m Trial 44 pruned. Trial was pruned at epoch 11.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:34:09,838]\u001b[0m Trial 45 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:34:25,712]\u001b[0m Trial 46 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:34:36,550]\u001b[0m Trial 47 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:34:52,213]\u001b[0m Trial 48 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:34:58,966]\u001b[0m Trial 49 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:34:58,966]\u001b[0m A new study created in memory with name: no-name-32c3a290-4337-4744-942e-8ce145ffbd9a\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:36:23,927]\u001b[0m Trial 0 finished with value: 44.386959075927734 and parameters: {'time_step': 11, 'n_layers': 3, 'kernel_size': 3, 'filter_size': 64, 'learning_rate': 0.009705432603580304, 'eta_decay': 0.5361169251682633}. Best is trial 0 with value: 44.386959075927734.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:37:48,194]\u001b[0m Trial 1 finished with value: 44.387088775634766 and parameters: {'time_step': 14, 'n_layers': 4, 'kernel_size': 3, 'filter_size': 16, 'learning_rate': 0.0752853501876878, 'eta_decay': 0.8192271386222199}. Best is trial 0 with value: 44.386959075927734.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:41:38,475]\u001b[0m Trial 2 finished with value: 8.492735862731934 and parameters: {'time_step': 24, 'n_layers': 3, 'kernel_size': 3, 'filter_size': 16, 'learning_rate': 0.0033369726009749275, 'eta_decay': 0.6933913300311221}. Best is trial 2 with value: 8.492735862731934.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:46:55,637]\u001b[0m Trial 3 finished with value: 38.23253631591797 and parameters: {'time_step': 1, 'n_layers': 4, 'kernel_size': 4, 'filter_size': 64, 'learning_rate': 3.208085869436045e-05, 'eta_decay': 0.7274648821534797}. Best is trial 2 with value: 8.492735862731934.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:48:38,315]\u001b[0m Trial 4 finished with value: 44.38733673095703 and parameters: {'time_step': 1, 'n_layers': 4, 'kernel_size': 2, 'filter_size': 128, 'learning_rate': 0.0476736948632005, 'eta_decay': 0.7519067306057658}. Best is trial 2 with value: 8.492735862731934.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:50:55,856]\u001b[0m Trial 5 pruned. Trial was pruned at epoch 28.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:51:56,092]\u001b[0m Trial 6 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:52:20,387]\u001b[0m Trial 7 pruned. Trial was pruned at epoch 32.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 02:52:44,595]\u001b[0m Trial 8 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:05:24,550]\u001b[0m Trial 9 finished with value: 6.056889057159424 and parameters: {'time_step': 23, 'n_layers': 3, 'kernel_size': 2, 'filter_size': 64, 'learning_rate': 0.000299694996081338, 'eta_decay': 0.5495655374685005}. Best is trial 9 with value: 6.056889057159424.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:15:37,511]\u001b[0m Trial 10 pruned. Trial was pruned at epoch 34.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:16:43,387]\u001b[0m Trial 11 pruned. Trial was pruned at epoch 76.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:18:56,911]\u001b[0m Trial 12 finished with value: 6.1243791580200195 and parameters: {'time_step': 25, 'n_layers': 2, 'kernel_size': 4, 'filter_size': 16, 'learning_rate': 0.002448728698826864, 'eta_decay': 0.6349695506643839}. Best is trial 9 with value: 6.056889057159424.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:19:31,924]\u001b[0m Trial 13 pruned. Trial was pruned at epoch 28.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:20:46,879]\u001b[0m Trial 14 finished with value: 5.654911994934082 and parameters: {'time_step': 20, 'n_layers': 2, 'kernel_size': 4, 'filter_size': 16, 'learning_rate': 0.003651651966094784, 'eta_decay': 0.5098969448635651}. Best is trial 14 with value: 5.654911994934082.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:23:09,858]\u001b[0m Trial 15 finished with value: 7.321180820465088 and parameters: {'time_step': 18, 'n_layers': 2, 'kernel_size': 2, 'filter_size': 64, 'learning_rate': 0.01520596355581346, 'eta_decay': 0.5098492688176325}. Best is trial 14 with value: 5.654911994934082.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:24:20,398]\u001b[0m Trial 16 pruned. Trial was pruned at epoch 9.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:29:58,743]\u001b[0m Trial 17 pruned. Trial was pruned at epoch 30.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:31:08,838]\u001b[0m Trial 18 pruned. Trial was pruned at epoch 77.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:31:25,972]\u001b[0m Trial 19 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:31:33,171]\u001b[0m Trial 20 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:31:41,073]\u001b[0m Trial 21 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:34:12,048]\u001b[0m Trial 22 finished with value: 6.017061710357666 and parameters: {'time_step': 32, 'n_layers': 2, 'kernel_size': 4, 'filter_size': 16, 'learning_rate': 0.0018220610991898636, 'eta_decay': 0.5658041142964975}. Best is trial 14 with value: 5.654911994934082.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:34:27,588]\u001b[0m Trial 23 pruned. Trial was pruned at epoch 3.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-20 03:34:35,866]\u001b[0m Trial 24 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:35:19,966]\u001b[0m Trial 25 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:35:28,846]\u001b[0m Trial 26 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:35:56,887]\u001b[0m Trial 27 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:36:07,215]\u001b[0m Trial 28 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:36:50,159]\u001b[0m Trial 29 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:37:15,623]\u001b[0m Trial 30 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:39:02,124]\u001b[0m Trial 31 finished with value: 5.9786295890808105 and parameters: {'time_step': 28, 'n_layers': 2, 'kernel_size': 4, 'filter_size': 16, 'learning_rate': 0.0025172594278320163, 'eta_decay': 0.6277167572972111}. Best is trial 14 with value: 5.654911994934082.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:41:25,300]\u001b[0m Trial 32 finished with value: 5.384769439697266 and parameters: {'time_step': 28, 'n_layers': 2, 'kernel_size': 4, 'filter_size': 16, 'learning_rate': 0.004694111161393867, 'eta_decay': 0.6091053583680488}. Best is trial 32 with value: 5.384769439697266.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:42:04,017]\u001b[0m Trial 33 pruned. Trial was pruned at epoch 36.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:42:13,237]\u001b[0m Trial 34 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:44:08,203]\u001b[0m Trial 35 finished with value: 5.601731777191162 and parameters: {'time_step': 27, 'n_layers': 2, 'kernel_size': 4, 'filter_size': 16, 'learning_rate': 0.008406597062532233, 'eta_decay': 0.5844642423690293}. Best is trial 32 with value: 5.384769439697266.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:44:47,111]\u001b[0m Trial 36 pruned. Trial was pruned at epoch 35.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:46:25,791]\u001b[0m Trial 37 finished with value: 6.117352485656738 and parameters: {'time_step': 26, 'n_layers': 2, 'kernel_size': 4, 'filter_size': 16, 'learning_rate': 0.008137099297189114, 'eta_decay': 0.6629777389846828}. Best is trial 32 with value: 5.384769439697266.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:46:45,440]\u001b[0m Trial 38 pruned. Trial was pruned at epoch 28.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:47:10,010]\u001b[0m Trial 39 pruned. Trial was pruned at epoch 9.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:47:34,648]\u001b[0m Trial 40 pruned. Trial was pruned at epoch 28.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:48:13,712]\u001b[0m Trial 41 pruned. Trial was pruned at epoch 35.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:48:59,644]\u001b[0m Trial 42 pruned. Trial was pruned at epoch 36.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 03:49:09,176]\u001b[0m Trial 43 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 04:01:48,571]\u001b[0m Trial 44 finished with value: 5.397600173950195 and parameters: {'time_step': 27, 'n_layers': 2, 'kernel_size': 4, 'filter_size': 128, 'learning_rate': 0.0014937611977254455, 'eta_decay': 0.6424442067727295}. Best is trial 32 with value: 5.384769439697266.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 04:02:18,786]\u001b[0m Trial 45 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 04:02:47,368]\u001b[0m Trial 46 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 04:03:57,739]\u001b[0m Trial 47 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 04:07:32,317]\u001b[0m Trial 48 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 04:07:42,413]\u001b[0m Trial 49 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 04:07:42,413]\u001b[0m A new study created in memory with name: no-name-07fdaf37-9785-4841-bac8-cd628a4d6dad\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 04:11:04,586]\u001b[0m Trial 0 finished with value: 182.52455139160156 and parameters: {'time_step': 3, 'n_layers': 4, 'kernel_size': 4, 'filter_size': 32, 'learning_rate': 9.437315429541503e-05, 'eta_decay': 0.6334095074529381}. Best is trial 0 with value: 182.52455139160156.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 04:25:54,877]\u001b[0m Trial 1 finished with value: 216.79156494140625 and parameters: {'time_step': 25, 'n_layers': 2, 'kernel_size': 4, 'filter_size': 128, 'learning_rate': 5.55542974987983e-06, 'eta_decay': 0.7560317064346782}. Best is trial 0 with value: 182.52455139160156.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 04:38:13,218]\u001b[0m Trial 2 finished with value: 216.20921325683594 and parameters: {'time_step': 25, 'n_layers': 2, 'kernel_size': 2, 'filter_size': 128, 'learning_rate': 1.4090520111451963e-06, 'eta_decay': 0.7573095450659986}. Best is trial 0 with value: 182.52455139160156.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 04:39:37,256]\u001b[0m Trial 3 finished with value: 14.61492919921875 and parameters: {'time_step': 21, 'n_layers': 2, 'kernel_size': 4, 'filter_size': 16, 'learning_rate': 0.00254130476184173, 'eta_decay': 0.8516438649304967}. Best is trial 3 with value: 14.61492919921875.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 04:40:48,039]\u001b[0m Trial 4 finished with value: 18.686683654785156 and parameters: {'time_step': 16, 'n_layers': 2, 'kernel_size': 2, 'filter_size': 16, 'learning_rate': 0.03983883811924423, 'eta_decay': 0.8675310364256166}. Best is trial 3 with value: 14.61492919921875.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 06:39:01,091]\u001b[0m Trial 5 finished with value: 59.65913772583008 and parameters: {'time_step': 18, 'n_layers': 5, 'kernel_size': 3, 'filter_size': 128, 'learning_rate': 0.0027064673500268887, 'eta_decay': 0.5920389843146138}. Best is trial 3 with value: 14.61492919921875.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 06:39:13,624]\u001b[0m Trial 6 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 06:39:37,550]\u001b[0m Trial 7 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 06:40:09,265]\u001b[0m Trial 8 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 06:40:49,548]\u001b[0m Trial 9 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 06:49:53,089]\u001b[0m Trial 10 finished with value: 13.884061813354492 and parameters: {'time_step': 13, 'n_layers': 3, 'kernel_size': 4, 'filter_size': 64, 'learning_rate': 0.0024769332640895442, 'eta_decay': 0.895696998631104}. Best is trial 10 with value: 13.884061813354492.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 06:50:18,653]\u001b[0m Trial 11 pruned. Trial was pruned at epoch 5.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:05:26,185]\u001b[0m Trial 12 finished with value: 10.207880020141602 and parameters: {'time_step': 25, 'n_layers': 3, 'kernel_size': 3, 'filter_size': 64, 'learning_rate': 0.0023833758037751255, 'eta_decay': 0.7883606700158586}. Best is trial 12 with value: 10.207880020141602.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:09:46,784]\u001b[0m Trial 13 finished with value: 63.11894607543945 and parameters: {'time_step': 28, 'n_layers': 3, 'kernel_size': 3, 'filter_size': 64, 'learning_rate': 0.03985051344602026, 'eta_decay': 0.7855943169496196}. Best is trial 12 with value: 10.207880020141602.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:10:04,858]\u001b[0m Trial 14 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:12:25,340]\u001b[0m Trial 15 pruned. Trial was pruned at epoch 8.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:12:34,341]\u001b[0m Trial 16 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:27:49,527]\u001b[0m Trial 17 finished with value: 10.727137565612793 and parameters: {'time_step': 20, 'n_layers': 3, 'kernel_size': 4, 'filter_size': 64, 'learning_rate': 0.008826894170471042, 'eta_decay': 0.8204234554200063}. Best is trial 12 with value: 10.207880020141602.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:29:16,706]\u001b[0m Trial 18 pruned. Trial was pruned at epoch 6.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:31:41,401]\u001b[0m Trial 19 pruned. Trial was pruned at epoch 14.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:32:16,055]\u001b[0m Trial 20 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:32:41,138]\u001b[0m Trial 21 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:35:04,711]\u001b[0m Trial 22 pruned. Trial was pruned at epoch 40.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:36:06,856]\u001b[0m Trial 23 pruned. Trial was pruned at epoch 9.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:36:20,940]\u001b[0m Trial 24 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:37:19,893]\u001b[0m Trial 25 pruned. Trial was pruned at epoch 6.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:37:51,409]\u001b[0m Trial 26 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:37:59,994]\u001b[0m Trial 27 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:40:37,771]\u001b[0m Trial 28 pruned. Trial was pruned at epoch 55.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:43:04,846]\u001b[0m Trial 29 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:43:50,989]\u001b[0m Trial 30 pruned. Trial was pruned at epoch 9.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-20 07:43:58,350]\u001b[0m Trial 31 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:44:06,272]\u001b[0m Trial 32 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:44:14,405]\u001b[0m Trial 33 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:44:44,697]\u001b[0m Trial 34 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:45:31,760]\u001b[0m Trial 35 finished with value: 17.844440460205078 and parameters: {'time_step': 21, 'n_layers': 2, 'kernel_size': 4, 'filter_size': 16, 'learning_rate': 0.038560052099249925, 'eta_decay': 0.7617641455917583}. Best is trial 12 with value: 10.207880020141602.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:47:12,761]\u001b[0m Trial 36 pruned. Trial was pruned at epoch 9.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:47:28,986]\u001b[0m Trial 37 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:47:43,231]\u001b[0m Trial 38 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:47:52,434]\u001b[0m Trial 39 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:48:28,505]\u001b[0m Trial 40 pruned. Trial was pruned at epoch 5.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:50:14,201]\u001b[0m Trial 41 finished with value: 14.368810653686523 and parameters: {'time_step': 21, 'n_layers': 2, 'kernel_size': 4, 'filter_size': 16, 'learning_rate': 0.03481557540371128, 'eta_decay': 0.766672437729308}. Best is trial 12 with value: 10.207880020141602.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:51:03,125]\u001b[0m Trial 42 pruned. Trial was pruned at epoch 63.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:53:01,221]\u001b[0m Trial 43 finished with value: 12.392240524291992 and parameters: {'time_step': 24, 'n_layers': 2, 'kernel_size': 4, 'filter_size': 16, 'learning_rate': 0.02273700758880384, 'eta_decay': 0.7738724400474771}. Best is trial 12 with value: 10.207880020141602.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:53:40,311]\u001b[0m Trial 44 pruned. Trial was pruned at epoch 16.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:56:35,194]\u001b[0m Trial 45 finished with value: 12.104251861572266 and parameters: {'time_step': 34, 'n_layers': 2, 'kernel_size': 4, 'filter_size': 16, 'learning_rate': 0.014664476676022765, 'eta_decay': 0.7469673548478992}. Best is trial 12 with value: 10.207880020141602.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 07:59:52,213]\u001b[0m Trial 46 finished with value: 10.061317443847656 and parameters: {'time_step': 40, 'n_layers': 2, 'kernel_size': 4, 'filter_size': 16, 'learning_rate': 0.015895322209927152, 'eta_decay': 0.7432401954207902}. Best is trial 46 with value: 10.061317443847656.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 08:01:41,887]\u001b[0m Trial 47 finished with value: 14.626704216003418 and parameters: {'time_step': 38, 'n_layers': 2, 'kernel_size': 3, 'filter_size': 16, 'learning_rate': 0.017501308255307428, 'eta_decay': 0.6658987995157944}. Best is trial 46 with value: 10.061317443847656.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 08:02:56,616]\u001b[0m Trial 48 pruned. Trial was pruned at epoch 61.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 08:05:39,953]\u001b[0m Trial 49 finished with value: 13.712568283081055 and parameters: {'time_step': 40, 'n_layers': 2, 'kernel_size': 4, 'filter_size': 16, 'learning_rate': 0.011435072809646288, 'eta_decay': 0.718032202098483}. Best is trial 46 with value: 10.061317443847656.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 08:05:39,953]\u001b[0m A new study created in memory with name: no-name-d5bfadf3-190a-4e8d-bfb2-c07874ed44f0\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 08:36:19,504]\u001b[0m Trial 0 finished with value: 375.7426452636719 and parameters: {'time_step': 25, 'n_layers': 3, 'kernel_size': 2, 'filter_size': 128, 'learning_rate': 1.0997362874397419e-05, 'eta_decay': 0.7295361305433553}. Best is trial 0 with value: 375.7426452636719.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 09:02:58,278]\u001b[0m Trial 1 finished with value: 27.211545944213867 and parameters: {'time_step': 27, 'n_layers': 5, 'kernel_size': 4, 'filter_size': 32, 'learning_rate': 0.00016332757795978542, 'eta_decay': 0.580620010567473}. Best is trial 1 with value: 27.211545944213867.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 09:06:02,172]\u001b[0m Trial 2 finished with value: 398.125 and parameters: {'time_step': 38, 'n_layers': 2, 'kernel_size': 4, 'filter_size': 16, 'learning_rate': 2.2151214803320727e-06, 'eta_decay': 0.6739688907203698}. Best is trial 1 with value: 27.211545944213867.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 09:07:09,752]\u001b[0m Trial 3 finished with value: 13.375675201416016 and parameters: {'time_step': 21, 'n_layers': 2, 'kernel_size': 2, 'filter_size': 16, 'learning_rate': 0.00807008727141731, 'eta_decay': 0.5393260732469418}. Best is trial 3 with value: 13.375675201416016.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 09:42:34,509]\u001b[0m Trial 4 finished with value: 332.29034423828125 and parameters: {'time_step': 25, 'n_layers': 3, 'kernel_size': 4, 'filter_size': 128, 'learning_rate': 1.8921859971500665e-05, 'eta_decay': 0.6818583533513669}. Best is trial 3 with value: 13.375675201416016.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 09:50:14,624]\u001b[0m Trial 5 finished with value: 11.190423011779785 and parameters: {'time_step': 17, 'n_layers': 4, 'kernel_size': 3, 'filter_size': 16, 'learning_rate': 0.01076645790546305, 'eta_decay': 0.517115816882165}. Best is trial 5 with value: 11.190423011779785.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 09:54:35,966]\u001b[0m Trial 6 finished with value: 7.23374080657959 and parameters: {'time_step': 12, 'n_layers': 2, 'kernel_size': 4, 'filter_size': 128, 'learning_rate': 0.032933732309459514, 'eta_decay': 0.687733880352575}. Best is trial 6 with value: 7.23374080657959.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 09:55:55,029]\u001b[0m Trial 7 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 09:57:20,060]\u001b[0m Trial 8 finished with value: 15.60898208618164 and parameters: {'time_step': 9, 'n_layers': 3, 'kernel_size': 2, 'filter_size': 32, 'learning_rate': 0.005447836043237685, 'eta_decay': 0.7460193404317119}. Best is trial 6 with value: 7.23374080657959.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:00:36,741]\u001b[0m Trial 9 finished with value: 21.975435256958008 and parameters: {'time_step': 8, 'n_layers': 4, 'kernel_size': 2, 'filter_size': 64, 'learning_rate': 0.020779455946050494, 'eta_decay': 0.6837409140735075}. Best is trial 6 with value: 7.23374080657959.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:02:22,311]\u001b[0m Trial 10 finished with value: 14.106290817260742 and parameters: {'time_step': 4, 'n_layers': 2, 'kernel_size': 3, 'filter_size': 128, 'learning_rate': 0.09644082494170396, 'eta_decay': 0.8968967831662291}. Best is trial 6 with value: 7.23374080657959.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:02:40,634]\u001b[0m Trial 11 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:05:58,983]\u001b[0m Trial 12 finished with value: 21.756175994873047 and parameters: {'time_step': 15, 'n_layers': 4, 'kernel_size': 3, 'filter_size': 64, 'learning_rate': 0.09300901706158175, 'eta_decay': 0.610721635275495}. Best is trial 6 with value: 7.23374080657959.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:06:18,324]\u001b[0m Trial 13 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:06:36,656]\u001b[0m Trial 14 pruned. Trial was pruned at epoch 6.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:10:45,641]\u001b[0m Trial 15 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:11:37,826]\u001b[0m Trial 16 finished with value: 10.186298370361328 and parameters: {'time_step': 10, 'n_layers': 2, 'kernel_size': 4, 'filter_size': 16, 'learning_rate': 0.020965997707058575, 'eta_decay': 0.6442184788967479}. Best is trial 6 with value: 7.23374080657959.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:13:17,901]\u001b[0m Trial 17 finished with value: 9.74042797088623 and parameters: {'time_step': 10, 'n_layers': 2, 'kernel_size': 4, 'filter_size': 64, 'learning_rate': 0.03623750250328465, 'eta_decay': 0.6397027493526761}. Best is trial 6 with value: 7.23374080657959.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:13:25,613]\u001b[0m Trial 18 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:13:35,592]\u001b[0m Trial 19 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:16:27,907]\u001b[0m Trial 20 pruned. Trial was pruned at epoch 27.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:20:00,217]\u001b[0m Trial 21 finished with value: 8.463592529296875 and parameters: {'time_step': 10, 'n_layers': 2, 'kernel_size': 4, 'filter_size': 128, 'learning_rate': 0.027742105433579686, 'eta_decay': 0.6391925133903626}. Best is trial 6 with value: 7.23374080657959.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:20:13,722]\u001b[0m Trial 22 pruned. Trial was pruned at epoch 5.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:25:01,291]\u001b[0m Trial 23 finished with value: 7.019050598144531 and parameters: {'time_step': 12, 'n_layers': 2, 'kernel_size': 4, 'filter_size': 128, 'learning_rate': 0.011770559372480843, 'eta_decay': 0.7102627239875937}. Best is trial 23 with value: 7.019050598144531.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-20 10:26:00,285]\u001b[0m Trial 24 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:26:17,050]\u001b[0m Trial 25 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:26:24,184]\u001b[0m Trial 26 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:27:14,123]\u001b[0m Trial 27 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:30:02,888]\u001b[0m Trial 28 finished with value: 11.640640258789062 and parameters: {'time_step': 12, 'n_layers': 2, 'kernel_size': 4, 'filter_size': 128, 'learning_rate': 0.07330266203555588, 'eta_decay': 0.665461759190956}. Best is trial 23 with value: 7.019050598144531.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:30:25,706]\u001b[0m Trial 29 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:30:55,104]\u001b[0m Trial 30 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:34:04,651]\u001b[0m Trial 31 finished with value: 7.584063529968262 and parameters: {'time_step': 10, 'n_layers': 2, 'kernel_size': 4, 'filter_size': 64, 'learning_rate': 0.03785718723504038, 'eta_decay': 0.6209317629507044}. Best is trial 23 with value: 7.019050598144531.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:34:16,592]\u001b[0m Trial 32 pruned. Trial was pruned at epoch 9.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:34:29,974]\u001b[0m Trial 33 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:34:38,820]\u001b[0m Trial 34 pruned. Trial was pruned at epoch 5.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:35:34,216]\u001b[0m Trial 35 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:37:34,196]\u001b[0m Trial 36 finished with value: 6.555112361907959 and parameters: {'time_step': 14, 'n_layers': 2, 'kernel_size': 4, 'filter_size': 32, 'learning_rate': 0.010961828659258432, 'eta_decay': 0.6550672427108138}. Best is trial 36 with value: 6.555112361907959.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:37:44,278]\u001b[0m Trial 37 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:37:58,079]\u001b[0m Trial 38 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:40:38,480]\u001b[0m Trial 39 finished with value: 8.075827598571777 and parameters: {'time_step': 32, 'n_layers': 2, 'kernel_size': 3, 'filter_size': 32, 'learning_rate': 0.013093559471227265, 'eta_decay': 0.6580351929689916}. Best is trial 36 with value: 6.555112361907959.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:40:54,850]\u001b[0m Trial 40 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:41:15,362]\u001b[0m Trial 41 pruned. Trial was pruned at epoch 8.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:41:28,927]\u001b[0m Trial 42 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:41:41,180]\u001b[0m Trial 43 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:41:55,925]\u001b[0m Trial 44 pruned. Trial was pruned at epoch 8.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:42:05,976]\u001b[0m Trial 45 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:43:04,948]\u001b[0m Trial 46 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:43:10,854]\u001b[0m Trial 47 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:43:25,764]\u001b[0m Trial 48 pruned. Trial was pruned at epoch 3.\u001b[0m\n",
      "\u001b[32m[I 2022-03-20 10:43:40,021]\u001b[0m Trial 49 pruned. Trial was pruned at epoch 3.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_name = ['mlp', 'lstm', 'tcn']\n",
    "objective = ['mse', 'quantile']\n",
    "quantile_list = [0.1, 0.5, 0.9]\n",
    "quantile_study = {}\n",
    "\n",
    "for model in model_name:\n",
    "    for i, var in enumerate(quantile_list):\n",
    "        current_objective = 'quantile'\n",
    "        current_model = model\n",
    "        quantile = var\n",
    "        #create study (default sampler = TPE, objective = quantile) \n",
    "        pruner = optuna.pruners.MedianPruner(n_startup_trials = 5, n_warmup_steps =3)\n",
    "        study = optuna.create_study(pruner=pruner, direction ='minimize')\n",
    "        study.optimize(optuna_objective, n_trials = 50);\n",
    "        quantile_study[model + 'quantile' + str(var)] = study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ee532e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      " \n",
      " mlpquantile0.1\n",
      "\n",
      " Best Loss: \n",
      " \n",
      " 8.096294403076172\n",
      "\n",
      " Hyperparameters: \n",
      " \n",
      " {'n_layers': 3, 'reg_alpha': 6.212514791077957e-06, 'reg_lambda': 2.326652392916294e-06, 'dropout_rate': 0.10456554658254263, 'n_units_0': 256, 'n_units_1': 128, 'n_units_2': 32, 'learning_rate': 0.0219363399910911, 'eta_decay': 0.8507050517321648}\n",
      "\n",
      "-------------------\n",
      "Name: \n",
      " \n",
      " mlpquantile0.5\n",
      "\n",
      " Best Loss: \n",
      " \n",
      " 16.015689849853516\n",
      "\n",
      " Hyperparameters: \n",
      " \n",
      " {'n_layers': 2, 'reg_alpha': 0.0003292624191114745, 'reg_lambda': 2.710160495505777e-05, 'dropout_rate': 0.29060862233655127, 'n_units_0': 128, 'n_units_1': 64, 'learning_rate': 0.004381503682976994, 'eta_decay': 0.6545573090226162}\n",
      "\n",
      "-------------------\n",
      "Name: \n",
      " \n",
      " mlpquantile0.9\n",
      "\n",
      " Best Loss: \n",
      " \n",
      " 9.194951057434082\n",
      "\n",
      " Hyperparameters: \n",
      " \n",
      " {'n_layers': 2, 'reg_alpha': 8.111842828761397e-06, 'reg_lambda': 8.485504099805394e-06, 'dropout_rate': 0.2635141652611792, 'n_units_0': 256, 'n_units_1': 256, 'learning_rate': 0.0008705444763254024, 'eta_decay': 0.712438951614092}\n",
      "\n",
      "-------------------\n",
      "Name: \n",
      " \n",
      " lstmquantile0.1\n",
      "\n",
      " Best Loss: \n",
      " \n",
      " 5.007742404937744\n",
      "\n",
      " Hyperparameters: \n",
      " \n",
      " {'time_step': 39, 'n_layers': 3, 'reg_alpha': 1.5759394308878862e-05, 'reg_lambda': 1.393516019160741e-06, 'n_units_0': 16, 'n_units_1': 32, 'n_units_2': 32, 'learning_rate': 0.006509824301581621, 'eta_decay': 0.7418381989622185}\n",
      "\n",
      "-------------------\n",
      "Name: \n",
      " \n",
      " lstmquantile0.5\n",
      "\n",
      " Best Loss: \n",
      " \n",
      " 17.07394027709961\n",
      "\n",
      " Hyperparameters: \n",
      " \n",
      " {'time_step': 20, 'n_layers': 2, 'reg_alpha': 0.0010869097413095485, 'reg_lambda': 0.0067339907486725446, 'n_units_0': 32, 'n_units_1': 64, 'learning_rate': 0.0038193996327849826, 'eta_decay': 0.7048466178996166}\n",
      "\n",
      "-------------------\n",
      "Name: \n",
      " \n",
      " lstmquantile0.9\n",
      "\n",
      " Best Loss: \n",
      " \n",
      " 6.290726184844971\n",
      "\n",
      " Hyperparameters: \n",
      " \n",
      " {'time_step': 29, 'n_layers': 1, 'reg_alpha': 9.860801384060622e-05, 'reg_lambda': 1.0728431920946264e-06, 'n_units_0': 32, 'learning_rate': 0.015242410037401302, 'eta_decay': 0.77582813121938}\n",
      "\n",
      "-------------------\n",
      "Name: \n",
      " \n",
      " tcnquantile0.1\n",
      "\n",
      " Best Loss: \n",
      " \n",
      " 5.384769439697266\n",
      "\n",
      " Hyperparameters: \n",
      " \n",
      " {'time_step': 28, 'n_layers': 2, 'kernel_size': 4, 'filter_size': 16, 'learning_rate': 0.004694111161393867, 'eta_decay': 0.6091053583680488}\n",
      "\n",
      "-------------------\n",
      "Name: \n",
      " \n",
      " tcnquantile0.5\n",
      "\n",
      " Best Loss: \n",
      " \n",
      " 10.061317443847656\n",
      "\n",
      " Hyperparameters: \n",
      " \n",
      " {'time_step': 40, 'n_layers': 2, 'kernel_size': 4, 'filter_size': 16, 'learning_rate': 0.015895322209927152, 'eta_decay': 0.7432401954207902}\n",
      "\n",
      "-------------------\n",
      "Name: \n",
      " \n",
      " tcnquantile0.9\n",
      "\n",
      " Best Loss: \n",
      " \n",
      " 6.555112361907959\n",
      "\n",
      " Hyperparameters: \n",
      " \n",
      " {'time_step': 14, 'n_layers': 2, 'kernel_size': 4, 'filter_size': 32, 'learning_rate': 0.010961828659258432, 'eta_decay': 0.6550672427108138}\n",
      "\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "for name, key in quantile_study.items():\n",
    "    print('Name: \\n \\n', name)\n",
    "    print('\\n Best Loss: \\n \\n', key.best_trial.value)\n",
    "    print('\\n Hyperparameters: \\n \\n', key.best_trial.params)\n",
    "    print('\\n-------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
